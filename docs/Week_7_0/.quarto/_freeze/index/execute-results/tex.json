{
  "hash": "fba2944db7e955614fd2a249b147db1e",
  "result": {
    "markdown": "---\ntitle: \"Week 7: Generalised Linear Models\"\nformat:\n  pdf:\n    latex-auto-install: true\n  html:    \n    code-link: true\n    code-fold: true\n    code-tools:\n      source: false\n      toggle: true\n    toc: true\n    toc-location: left\n    toc-title: Contents\n    number-sections: true\neditor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n```{=html}\n<style>\np.caption {\n  font-size: 1.4em;\n}\n</style>\n```\n\n# Introduction\n\nIn Weeks 3 and 4 we looked at modelling data using linear regression models were we had:\n\n-   a **continuous response variable** $y$ and\n-   one or more **explanatory variables** $x_1, x_2,\\ldots, x_p$, which were **numerical**/**categorical** variables.\n\nRecall that for data $(y_i, x_i), ~ i = 1,\\ldots, n$, where $y$ is a continuous response variable, we can write a simple linear regression model as follows:\n\n$$y_i = \\alpha + \\beta x_i + \\epsilon_i, ~~~~ \\epsilon_i \\sim N(0, \\sigma^2),$$ where\n\n-   $y_i$ is the $i^{th}$ observation of the continuous response variable;\n-   $\\alpha$ is the **intercept** of the regression line;\n-   $\\beta$ is the **slope** of the regression line;\n-   $x_i$ is the $i^{th}$ observation of the explanatory variable; and\n-   $\\epsilon_i$ is the $i^{th}$ random component.\n\nThus, the full probability model for $y_i$ given $x_i$ ($y_i | x_i$) can be written as\n\n$$y_i | x_i \\sim N(\\alpha + \\beta x_i, \\sigma^2),$$\n\nwhere the mean $\\alpha + \\beta x_i$ is given by the deterministic part of the model and the variance $\\sigma^2$ by the random part. Hence we make the assumption that the outcomes $y_i$ are normally distributed with mean $\\alpha + \\beta x_i$ and variance $\\sigma^2$. However, what if our response variable $y$ is not a continuous random variable?\n\n## Generalised linear models\n\nThe main objective this week is to introduce **Generalised Linear Models (GLMs)**, which extend the linear model framework to response variables that don't follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:\n\n\\vspace{-0.5cm}\n\n\n```{=tex}\n\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\n```\n\nwhere the response $y_i$ is predicted through the linear combination $\\boldsymbol{\\mu}_i$ of explanatory variables by the link function $g(\\cdot)$, assuming some distribution $f(\\cdot)$ for $y_i$, and $\\mathbf{x}_i^\\top$ is the $i^{th}$ row of the design matrix $\\boldsymbol{X}$. For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as $f(\\cdot)$, with corresponding link function equal to the Identity function, that is, $g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i$.\n\nWhat if our response variable $y$ is binary (e.g. yes/no, success/failure, alive/dead)? That is, the independent responses $y_i$ can either be:\n\n-   **binary**, taking the value 1 (say success, with probability $p_i$) or 0 (failure, with probability $1-p_i$) or\n\n-   **binomial**, where $y_i$ is the number of successes in a given number of trials $n_i$, with the probability of success being $p_i$ and the probability of failure being $1-p_i$.\n\nIn both cases the distribution of $y_i$ is assumed to be binomial, but in the first case it is Bin$(1,p_i)$ and in the second case it is Bin$(n_i,p_i)$. Hence, a binary response variable $y_i$ has a binomial distribution with corresponding link function $g(\\cdot)$ equal to the **logit link** function, that is\n\n$$g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),$$ which is also referred to as the **log-odds** (since $p_i ~ / ~ 1-p_i$ is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success $p_i$, and as we know probabilities must be between 0 and 1 $\\left(p_i \\in [0, 1]\\right)$. So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\n$$p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},$$ we would need to ensure that in some way $\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]$, that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\n$$\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},$$\n\nno restrictions need to be in place on our estimates of the parameter vector $\\boldsymbol{\\beta}$, since the inverse of the logit link function will always gives us valid probabilities since\n\n$$p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].$$ This linear regression model with a binary response variable is referred to as **logistic regression**. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n| **Model** |                   **Random component**                    |                               **Systematic component**                                |                 **Link function**                  |\n|:---------:|:---------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:--------------------------------------------------:|\n|  Normal   | $y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),$ | $\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots$ |                  $g(\\mu_i)=\\mu_i$                  |\n| Logistic  |    $y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),$     | $\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots$  | $g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)$ |\n\n## Required R packages {.unnumbered}\n\nBefore we proceed, load all the packages needed for this week:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(gapminder)\nlibrary(sjPlot)\nlibrary(stats)\nlibrary(readr)\nlibrary(janitor)\nlibrary(tidymodels)\n```\n:::\n\n\n# Logistic regression with one numerical explanatory variable\n\nHere we shall begin by fitting a logistic regression model with one numerical explanatory variable. Let's return to the `evals` data from the `moderndive` package that we examined in Week 3.\n\n## Teaching evaluation scores\n\nStudent feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see [Economics of Education Review](https://www.journals.elsevier.com/economics-of-education-review/) for details. Here, we shall look at a study from student evaluations of $n=463$ professors from The University of Texas at Austin.\n\nPreviously, we looked at **teaching score** as our continuous response variable and **beauty score** as our explanatory variable. Now we shall consider **gender** as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in **gender** by **age** of the teaching instructors within the `evals` data set.\n\nFirst, let's start by selecting the variables of interest from the `evals` data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.gender <- evals |>\n                  select(gender, age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  gender   age\n  <fct>  <int>\n1 female    36\n2 female    36\n3 female    36\n4 female    36\n5 male      59\n6 male      59\n```\n:::\n:::\n\n\nNow, let's look at a boxplot of `age` by `gender` to get an initial impression of the data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = evals.gender, aes(x = gender, y = age, fill = gender)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Age\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Teaching instructor age by gender.](index_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n\n\nHere we can see that the age of male teaching instructors tends to be higher than that of their female counterparts. Now, let's fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female.\n\n## Log-odds\n\nTo fit a logistic regression model we will use the generalised linear model function `glm`, which acts in a very similar manner to the `lm` function we have used previously. We only have to deal with an additional argument. The logistic regression model with **gender** as the response and **age** as the explanatory variable is given by:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel <- logistic_reg() |> \n  set_engine(\"glm\")\nmodel <- model |> \n  fit(gender ~ age, data = evals.gender) |>\n  extract_fit_engine()\n```\n:::\n\n\nHere we include the additional `family` argument, which states the distribution and link function we would like to use. Hence `family = binomial(link = \"logit\")` states we have a binary response variable, and thus have a binomial distribution, with its corresponding **logit link** function. Now, let's take a look at the summary produced from our logistic regression model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::glm(formula = gender ~ age, family = stats::binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7134  -1.1815   0.7238   1.0180   1.4778  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.69795    0.51194  -5.270 1.36e-07 ***\nage          0.06296    0.01059   5.948 2.71e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 630.30  on 462  degrees of freedom\nResidual deviance: 591.41  on 461  degrees of freedom\nAIC: 595.41\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nFirstly, the baseline category for our binary response is `female`. This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the `levels` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevels(evals.gender$gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"female\" \"male\"  \n```\n:::\n:::\n\n\nThis means that estimates from the logistic regression model are for a change on the **log-odds** scale for `males` in comparison to the response baseline `females`. That is\n\n\n```{=tex}\n\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\cdot \\textrm{age} = -2.7 + 0.06 \\cdot \\textrm{age}, \\nonumber\n\\end{align}\n```\n\nwhere $p = \\textrm{Prob}\\left(\\textrm{Male}\\right)$ and $1 - p = \\textrm{Prob}\\left(\\textrm{Female}\\right)$. Hence, the **log-odds** of the instructor being male increase by 0.06 for every one unit increase in `age`. This provides us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod.coef.logodds <- model |>\n                      summary() |>\n                      coef()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nage.logodds.lower <- (mod.coef.logodds[\"age\", \"Estimate\"] \n                        - 1.96 * mod.coef.logodds[\"age\", \"Std. Error\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04221777\n```\n:::\n\n```{.r .cell-code}\nage.logodds.upper <- (mod.coef.logodds[\"age\", \"Estimate\"] \n                        + 1.96 * mod.coef.logodds[\"age\", \"Std. Error\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08371167\n```\n:::\n:::\n\n\nHence the point estimate for the log-odds is 0.06, which has a corresponding 95% confidence interval of (0.04, 0.08). This can be displayed graphically using the `plot_model` function from the `sjPlot` package by simply passing our `model` as an argument:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(model, show.values = TRUE, transform = NULL,\n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n```\n\n::: {.cell-output-display}\n![The log-odds of age for male instructors.](index_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n\n\nSome of the interesting arguments that can be passed to the `plot_model` function are:\n\n-   `show.values = TRUE/FALSE`: Whether the log-odds/odds values should be displayed;\n-   `show.p = TRUE/FALSE`: Adds asterisks that indicate the significance level of estimates to the value labels;\n-   `transform`: A character vector naming the function that will be applied to the estimates. The default transformation uses `exp` to display the odds ratios, while `transform = NULL` displays the log-odds; and\n-   `vline.color`: colour of the vertical \"zero effect\" line.\n\nFurther details on using `plot_model` can be found [here](https://strengejacke.wordpress.com/2017/10/23/one-function-to-rule-them-all-visualization-of-regression-models-in-rstats-w-sjplot/). Now, let's add the estimates of the log-odds to our data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.gender <- evals.gender |>\n                  mutate(logodds.male = predict(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n  gender   age logodds.male\n  <fct>  <int>        <dbl>\n1 female    36       -0.431\n2 female    36       -0.431\n3 female    36       -0.431\n4 female    36       -0.431\n5 male      59        1.02 \n6 male      59        1.02 \n```\n:::\n:::\n\n\n## Odds\n\nTypically we would like to work on the **odds** scale as it is easier to interpret an odds-ratio as opposed to the log-odds-ratio. To obtain the odds we simply exponentiate the log-odds, that is\n\n\n```{=tex}\n\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right), \\nonumber\n\\end{align}\n```\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel |>\n coef() |>\n  exp()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n 0.06734369  1.06498927 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nOn the odds scale, the value of the intercept (0.07) gives the odds of a teaching instructor being male given their `age = 0`, which is obviously not a viable age for a teaching instructor, and hence why this value is very close to zero. For `age` we have an odds of 1.06, which indicates that for every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of 1.06. So how is this calculated? Let's look at the odds-ratio obtained from instructors aged 51 and 52 years old, that is, a one unit difference:\n\n\n```{=tex}\n\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{age=52}}}{\\mbox{Odds}_{\\mbox{age=51}}} = \\left(\\frac{\\frac{p_{\\mbox{age=52}}}{1 - p_{\\mbox{age=52}}}}{\\frac{p_{\\mbox{age=51}}}{1 - p_{\\mbox{age=51}}}}\\right) = \\frac{\\exp\\left(\\alpha + \\beta \\cdot 52\\right)}{\\exp\\left(\\alpha + \\beta \\cdot 51\\right)} = \\exp\\left(\\beta \\cdot (52 - 51)\\right) = \\exp\\left(0.06\\right) = 1.06. \\nonumber\n\\end{align}\n```\n\nFor example, the odds of a teaching instructor who is 45 years old being male is given by\n\n\n```{=tex}\n\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age}\\right) = \\exp\\left(-2.7 + 0.06 \\cdot 45\\right) = 1.15. \\nonumber\n\\end{align}\n```\n\nThis can be interpreted as the chances of an instructor who is 45 being male are 15% greater than them being female. We can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of our log-odds interval:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nage.odds.lower <- exp(age.logodds.lower)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.043122\n```\n:::\n\n```{.r .cell-code}\nage.odds.upper <- exp(age.logodds.upper)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.087315\n```\n:::\n:::\n\n\nHence the point estimate for the odds is 1.06, which has a corresponding 95% confidence interval of (1.04, 1.09). This can be displayed graphically using the `plot_model` function from the `sjPlot` package by simply passing our `model` as an argument as well as removing `transform = NULL` (the default transformation is exponential):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(model, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE, axis.lim = c(1, 1.5))\n```\n\n::: {.cell-output-display}\n![The odds of age for male instructors.](index_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n\n\n**Note**: `axis.lim` is used to zoom in on the 95% confidence interval. The `confint()` function can also be used to compute confidence intervals (`confint(model)` for example).\n\nNow, let's add the estimates of the odds to our data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.gender <- evals.gender |>\n                  mutate(odds.male = exp(logodds.male))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  gender   age logodds.male odds.male\n  <fct>  <int>        <dbl>     <dbl>\n1 female    36       -0.431     0.650\n2 female    36       -0.431     0.650\n3 female    36       -0.431     0.650\n4 female    36       -0.431     0.650\n5 male      59        1.02      2.76 \n6 male      59        1.02      2.76 \n```\n:::\n:::\n\n\n## Probabilities\n\nWe can obtain the probability $p = \\textrm{Prob}(\\textrm{Male})$ using the following transformation:\n\n\n```{=tex}\n\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}. \\nonumber\n\\end{align}\n```\n\nFor example, the probability of a teaching instructor who is 52 years old being male is\n\n\n```{=tex}\n\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}\n=\\frac{\\exp\\left(-2.697946 + 0.0629647\\cdot 52 \\right)}{1 + \\exp\\left(-2.697946 + 0.0629647\\cdot 52 \\right)} \n= 0.64, \\nonumber\n\\end{align}\n```\n\nwhich can be computed in R as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np.num <- (exp(mod.coef.logodds[\"(Intercept)\", \"Estimate\"] \n              + mod.coef.logodds[\"age\", \"Estimate\"] * 52))\np.denom <- 1 + p.num\np.num / p.denom\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6401971\n```\n:::\n:::\n\n\nThe `plogis()` function from the `stats` library can also be used to obtain probabilities from the log-odds:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplogis(mod.coef.logodds[\"(Intercept)\", \"Estimate\"] \n        + mod.coef.logodds[\"age\", \"Estimate\"] * 52)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6401971\n```\n:::\n:::\n\n\nLet's add the probabilities to our data, which is done using the `fitted()` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.gender <- evals.gender |>\n                  mutate(probs.male = fitted(model))\n```\n:::\n\n\n**Note**: `predict(model, type = \"response\")` will also provide the estimated probabilities.\n\nFinally, we can plot the probability of being male using the `geom_smooth()` function by giving `method = \"glm\"` and `methods.args = list(family = \"binomial\")` as follows:\n\n<!-- ```{r, echo = TRUE, eval = TRUE, fig.cap = \"Probability of teaching instructor being male by age.\", fig.align = \"center\", out.width = \"60%\"} -->\n\n<!-- ggplot(data = evals.gender, aes(x = age, y = probs.male)) + -->\n\n<!--   geom_dotplot(dotsize = 0.6, alpha = 0.2, aes(fill = gender), -->\n\n<!--                 binwidth = 1, stackgroups = TRUE) + -->\n\n<!--   geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) + -->\n\n<!--   labs(x = \"Age\", y = \"Probability of instructor being male\") -->\n\n<!-- ``` -->\n\n<!--  -->\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = evals.gender, aes(x = age, y = probs.male)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  labs(x = \"Age\", y = \"Probability of instructor being male\")\n```\n\n::: {.cell-output-display}\n![Probability of teaching instructor being male by age.](index_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-align='center' fig-pos='H' width=60%}\n:::\n:::\n\n\n<!-- **Note**: the ages of all teaching instructors have been superimposed as a dotplot using `geom_dotplot()`. -->\n\n\\newpage\n\nThe `plot_model()` function from the `sjPlot` package can also produce the estimated probabilities by `age` as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(model, type = \"pred\", title = \"\", terms=\"age [all]\", axis.title = c(\"Age\", \"Prob. of instructor being male\"))\n```\n\n::: {.cell-output-display}\n![Probability of teaching instructor being male by age.](index_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n\n\n# Logistic regression with one categorical explanatory variable\n\nInstead of having a numerical explanatory variable such as `age`, let's now use the binary categorical variable `ethnicity` as our explanatory variable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.ethnic <- evals |>\n                  select(gender, ethnicity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  gender ethnicity   \n  <fct>  <fct>       \n1 female minority    \n2 female minority    \n3 female minority    \n4 female minority    \n5 male   not minority\n6 male   not minority\n```\n:::\n:::\n\n\nNow, let's look at a barplot of the proportion of males and females by `ethnicity` to get an initial impression of the data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.ethnic |>\n  tabyl(ethnicity, gender) |>\n  adorn_percentages() |>\n  adorn_pct_formatting() |>\n  adorn_ns() # To show original counts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    ethnicity      female        male\n     minority 56.2%  (36) 43.8%  (28)\n not minority 39.8% (159) 60.2% (240)\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(evals.ethnic, aes(x = gender, group = ethnicity)) +\n    geom_bar(aes(y = after_stat(prop), fill = ethnicity), stat = \"count\", position = \"dodge\") +\n    labs(y = \"Proportion\", fill = \"Ethnicity\")\n```\n\n::: {.cell-output-display}\n![Barplot of teaching instructors' gender by ethnicity.](index_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-align='center' fig-pos='H' width=48%}\n:::\n:::\n\n\nWe can see that a larger proportion of instructors in the `minority` ethnic group are female (56.3% vs 43.8%), while the `not minority` ethnic group is comprised of more male instructors (60.2% vs 39.8%). Now we shall fit a logistic regression model to determine whether the gender of a teaching instructor can be predicted from their ethnicity.\n\n## Log-odds\n\nThe logistic regression model is given by:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel.ethnic <- glm(gender ~ ethnicity, data = evals.ethnic,\n                      family = binomial(link = \"logit\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel.ethnic |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = gender ~ ethnicity, family = binomial(link = \"logit\"), \n    data = evals.ethnic)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.357  -1.357   1.008   1.008   1.286  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)  \n(Intercept)            -0.2513     0.2520  -0.997   0.3186  \nethnicitynot minority   0.6630     0.2719   2.438   0.0148 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 630.30  on 462  degrees of freedom\nResidual deviance: 624.29  on 461  degrees of freedom\nAIC: 628.29\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nAgain, the baseline category for our binary response is `female`. Also, the baseline category for our explanatory variable is `minority`, which, like `gender`, is done alphabetically by default by R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevels(evals.ethnic$ethnicity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"minority\"     \"not minority\"\n```\n:::\n:::\n\n\nThis means that estimates from the logistic regression model are for a change on the **log-odds** scale for `males` ($p = \\textrm{Prob}(\\textrm{Males})$) in comparison to the response baseline `females`. That is\n\n\n```{=tex}\n\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\cdot \\textrm{ethnicity} = -0.25 + 0.66 \\cdot \\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority}), \\nonumber\n\\end{align}\n```\n\nwhere $\\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority})$ is an indicator function. Hence, the **log-odds** of an instructor being male increase by 0.66 if they are in the ethnicity group `not minority`. This provides us with a point estimate of how the log-odds changes with ethnicity, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod.ethnic.coef.logodds <- model.ethnic |>\n                            summary() |>\n                            coef()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nethnic.logodds.lower <- (mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"] \n                          - 1.96 *\n                           mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Std. Error\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1300587\n```\n:::\n\n```{.r .cell-code}\nethnic.logodds.upper <- (mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"] \n                          + 1.96 *\n                           mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Std. Error\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.19604\n```\n:::\n:::\n\n\nHence the point estimate for the log-odds is 0.66, which has a corresponding 95% confidence interval of (0.13, 1.2). This can be displayed graphically using the `plot_model` function from the `sjPlot` package by simply passing our `model` as an argument:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(model.ethnic, show.values = TRUE, transform = NULL, \n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n```\n\n::: {.cell-output-display}\n![The log-odds for male instructors by ethnicity (not a minority).](index_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n\n\nNow, let's add the estimates of the log-odds to our data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.ethnic <- evals.ethnic |>\n                  mutate(logodds.male = predict(model.ethnic))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n  gender ethnicity    logodds.male\n  <fct>  <fct>               <dbl>\n1 female minority           -0.251\n2 female minority           -0.251\n3 female minority           -0.251\n4 female minority           -0.251\n5 male   not minority        0.412\n6 male   not minority        0.412\n```\n:::\n:::\n\n\n## Odds\n\nOn the **odds** scale the regression coefficients are given by\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel.ethnic |>\n coef() |>\n  exp()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          (Intercept) ethnicitynot minority \n            0.7777778             1.9407008 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nThe `(Intercept)` gives us the odds of the instructor being male given that they are in the `minority` ethnic group, that is, 0.78 (the indicator function is zero in that case). The odds of the instructor being male given they are in the `not minority` ethnic group are 1.94 times greater than the odds if they were in the `minority` ethnic group.\n\nBefore moving on, let's take a look at how these values are computed. First, the odds of the instructor being male given that they are in the `minority` ethnic group can be obtained as follows:\n\n\\vspace{-0.6cm}\n\n\n```{=tex}\n\\begin{align}\n\\frac{p_{\\mbox{minority}}}{1 - p_{\\mbox{minority}}} = \\exp\\left(\\alpha\\right) = \\exp\\left(-0.25\\right) = 0.78. \\nonumber\n\\end{align}\n```\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# the number of instructors in the minority\npmin <- evals.ethnic |>\n              filter(ethnicity == \"minority\") |>\n              summarize(n()) |>\n              pull()\n\n# the number of male instructors in the minority\npmin.male <- evals.ethnic |>\n              filter(ethnicity == \"minority\", gender == \"male\") |>\n              summarize(n()) |>\n              pull()\n\n# the proportion/probability of males in the minority\nprob.min.male <- pmin.male / pmin\nodds.min.male <- prob.min.male / (1 - prob.min.male)\nodds.min.male\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7777778\n```\n:::\n:::\n\n\nNow, the odds-ratio of an instructor being male in the `not minority` compared to the `minority` ethnic group is found as follows:\n\n\\vspace{-0.6cm}\n\n\n```{=tex}\n\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{not minority}}}{\\mbox{Odds}_{\\mbox{minority}}} = \\frac{\\frac{p_{\\mbox{not minority}}}{1 - p_{\\mbox{not minority}}}}{\\frac{p_{\\mbox{minority}}}{1 - p_{\\mbox{minority}}}} &= \\frac{\\exp\\left(\\alpha + \\beta\\right)}{\\exp\\left(\\alpha\\right)} = \\exp\\left(\\alpha + \\beta - \\alpha\\right) = \\exp\\left(\\beta\\right) = \\exp\\left(0.66 \\right) = 1.93. \\nonumber \n\\end{align}\n```\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# the number of instructors not in the minority\npnotmin <- evals.ethnic |>\n              filter(ethnicity == \"not minority\") |>\n              summarize(n()) |>\n              pull()\n\n# the number of male instructors not in the minority\npnotmin.male <- evals.ethnic |>\n              filter(ethnicity == \"not minority\", gender == \"male\") |>\n              summarize(n()) |>\n              pull()\n\n# the proportion/probability of males not in the minority\nprob.notmin.male <- pnotmin.male / pnotmin\nodds.notmin.male <- prob.notmin.male / (1 - prob.notmin.male)\nodds.ratio.notmin <- odds.notmin.male / odds.min.male\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.940701\n```\n:::\n:::\n\n\nWe can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of the log-odds interval:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nethnic.odds.lower <- exp(ethnic.logodds.lower)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.138895\n```\n:::\n\n```{.r .cell-code}\nethnic.odds.upper <- exp(ethnic.logodds.upper)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.306994\n```\n:::\n:::\n\n\nHence the point estimate for the odds-ratio is 1.94, which has a corresponding 95% confidence interval of (1.14, 3.31). Again, we can display this graphically using the `plot_model` function from the `sjPlot` package:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(model.ethnic, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE)\n```\n\n::: {.cell-output-display}\n![The odds-ratio of a male instructor given they are in the `not minority` group.](index_files/figure-pdf/unnamed-chunk-29-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n\n\nNow, let's add the estimates of the odds to our data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.ethnic <- evals.ethnic |>\n                  mutate(odds.male = exp(logodds.male))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  gender ethnicity    logodds.male odds.male\n  <fct>  <fct>               <dbl>     <dbl>\n1 female minority           -0.251     0.778\n2 female minority           -0.251     0.778\n3 female minority           -0.251     0.778\n4 female minority           -0.251     0.778\n5 male   not minority        0.412     1.51 \n6 male   not minority        0.412     1.51 \n```\n:::\n:::\n\n\n## Probabilities\n\nThe probabilities of an instructor being male given they are in the `minority` and `not minority` groups are\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplogis(mod.ethnic.coef.logodds[\"(Intercept)\", \"Estimate\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4375\n```\n:::\n\n```{.r .cell-code}\nplogis(mod.ethnic.coef.logodds[\"(Intercept)\", \"Estimate\"] +\n         mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6015038\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nHence, the probabilities of an instructor being male given they are in the `minority` and `not minority` ethnic groups are 0.437 and 0.602, respectively.\n\nLet's add the probabilities to our data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals.ethnic <- evals.ethnic |>\n                  mutate(probs.male = fitted(model.ethnic))\n```\n:::\n\n\nFinally, we can use the `plot_model()` function from the `sjPlot` package to produce the estimated probabilities by `ethnicity` as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(model.ethnic, type = \"pred\", terms = \"ethnicity\", axis.title = c(\"Ethnicity\", \"Prob. of instructor being male\"), title = \" \")\n```\n\n::: {.cell-output-display}\n![Probability of teaching instructor being male by ethnicity.](index_files/figure-pdf/unnamed-chunk-34-1.pdf){fig-align='center' fig-pos='H' width=50%}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}