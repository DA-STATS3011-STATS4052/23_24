[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "In Weeks 3 and 4 we looked at modelling data using linear regression models were we had:\n\na continuous response variable \\(y\\) and\none or more explanatory variables \\(x_1, x_2,\\ldots, x_p\\), which were numerical/categorical variables.\n\nRecall that for data \\((y_i, x_i), ~ i = 1,\\ldots, n\\), where \\(y\\) is a continuous response variable, we can write a simple linear regression model as follows:\n\\[y_i = \\alpha + \\beta x_i + \\epsilon_i, ~~~~ \\epsilon_i \\sim N(0, \\sigma^2),\\] where\n\n\n\\(y_i\\) is the \\(i^{th}\\) observation of the continuous response variable;\n\n\\(\\alpha\\) is the intercept of the regression line;\n\n\\(\\beta\\) is the slope of the regression line;\n\n\\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and\n\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random component.\n\nThus, the full probability model for \\(y_i\\) given \\(x_i\\) (\\(y_i | x_i\\)) can be written as\n\\[y_i | x_i \\sim N(\\alpha + \\beta x_i, \\sigma^2),\\]\nwhere the mean \\(\\alpha + \\beta x_i\\) is given by the deterministic part of the model and the variance \\(\\sigma^2\\) by the random part. Hence we make the assumption that the outcomes \\(y_i\\) are normally distributed with mean \\(\\alpha + \\beta x_i\\) and variance \\(\\sigma^2\\). However, what if our response variable \\(y\\) is not a continuous random variable?\n\nThe main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nWhat if our response variable \\(y\\) is binary (e.g. yes/no, success/failure, alive/dead)? That is, the independent responses \\(y_i\\) can either be:\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\nbinomial, where \\(y_i\\) is the number of successes in a given number of trials \\(n_i\\), with the probability of success being \\(p_i\\) and the probability of failure being \\(1-p_i\\).\n\nIn both cases the distribution of \\(y_i\\) is assumed to be binomial, but in the first case it is Bin\\((1,p_i)\\) and in the second case it is Bin\\((n_i,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) equal to the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\] which is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\] This linear regression model with a binary response variable is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)\n\n\n\nBefore we proceed, load all the packages needed for this week:\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(gapminder)\nlibrary(sjPlot)\nlibrary(stats)\nlibrary(readr)\nlibrary(janitor)\nlibrary(tidymodels)"
  },
  {
    "objectID": "index.html#generalised-linear-models",
    "href": "index.html#generalised-linear-models",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "The main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nWhat if our response variable \\(y\\) is binary (e.g. yes/no, success/failure, alive/dead)? That is, the independent responses \\(y_i\\) can either be:\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\nbinomial, where \\(y_i\\) is the number of successes in a given number of trials \\(n_i\\), with the probability of success being \\(p_i\\) and the probability of failure being \\(1-p_i\\).\n\nIn both cases the distribution of \\(y_i\\) is assumed to be binomial, but in the first case it is Bin\\((1,p_i)\\) and in the second case it is Bin\\((n_i,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) equal to the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\] which is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\] This linear regression model with a binary response variable is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)"
  },
  {
    "objectID": "index.html#required-r-packages",
    "href": "index.html#required-r-packages",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "Before we proceed, load all the packages needed for this week:\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(gapminder)\nlibrary(sjPlot)\nlibrary(stats)\nlibrary(readr)\nlibrary(janitor)\nlibrary(tidymodels)"
  },
  {
    "objectID": "index.html#teaching-evaluation-scores",
    "href": "index.html#teaching-evaluation-scores",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.1 Teaching evaluation scores",
    "text": "2.1 Teaching evaluation scores\nStudent feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see Economics of Education Review for details. Here, we shall look at a study from student evaluations of \\(n=463\\) professors from The University of Texas at Austin.\nPreviously, we looked at teaching score as our continuous response variable and beauty score as our explanatory variable. Now we shall consider gender as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in gender by age of the teaching instructors within the evals data set.\nFirst, let’s start by selecting the variables of interest from the evals data set:\n\nCodeevals.gender &lt;- evals |&gt;\n                  select(gender, age)\n\n# A tibble: 6 × 2\n  gender   age\n  &lt;fct&gt;  &lt;int&gt;\n1 female    36\n2 female    36\n3 female    36\n4 female    36\n5 male      59\n6 male      59\n\n\nNow, let’s look at a boxplot of age by gender to get an initial impression of the data:\n\nCodeggplot(data = evals.gender, aes(x = gender, y = age, fill = gender)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\nTeaching instructor age by gender.\n\n\n\nHere we can see that the age of male teaching instructors tends to be higher than that of their female counterparts. Now, let’s fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female."
  },
  {
    "objectID": "index.html#log-odds",
    "href": "index.html#log-odds",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.2 Log-odds",
    "text": "2.2 Log-odds\nTo fit a logistic regression model we will use the generalised linear model function glm, which acts in a very similar manner to the lm function we have used previously. The logistic regression model with gender as the response and age as the explanatory variable is given by:\n\nCodemodel &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\nmodel &lt;- model |&gt; \n  fit(gender ~ age, data = evals.gender) |&gt;\n  extract_fit_engine()\n\n\nThis model uses the logit link function. Now, let’s take a look at the summary produced from our logistic regression model:\n\nCodemodel |&gt;\n  summary()\n\n\nCall:\nstats::glm(formula = gender ~ age, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.69795    0.51194  -5.270 1.36e-07 ***\nage          0.06296    0.01059   5.948 2.71e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 630.30  on 462  degrees of freedom\nResidual deviance: 591.41  on 461  degrees of freedom\nAIC: 595.41\n\nNumber of Fisher Scoring iterations: 4\n\n\nFirstly, the baseline category for our binary response is female. This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the levels function:\n\nCodelevels(evals.gender$gender)\n\n[1] \"female\" \"male\"  \n\n\nThis means that estimates from the logistic regression model are for a change on the log-odds scale for males in comparison to the response baseline females. That is\n\\[\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\cdot \\textrm{age} = -2.7 + 0.06 \\cdot \\textrm{age}, \\nonumber\n\\end{align}\\]\nwhere \\(p = \\textrm{Prob}\\left(\\textrm{Male}\\right)\\) and \\(1 - p = \\textrm{Prob}\\left(\\textrm{Female}\\right)\\). Hence, the log-odds of the instructor being male increase by 0.06 for every one unit increase in age. This provides us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:\n\nCodemod.coef.logodds &lt;- model |&gt;\n                      summary() |&gt;\n                      coef()\n\n\n\nCodeage.logodds.lower &lt;- (mod.coef.logodds[\"age\", \"Estimate\"] \n                        - 1.96 * mod.coef.logodds[\"age\", \"Std. Error\"])\n\n[1] 0.04221777\n\nCodeage.logodds.upper &lt;- (mod.coef.logodds[\"age\", \"Estimate\"] \n                        + 1.96 * mod.coef.logodds[\"age\", \"Std. Error\"])\n\n[1] 0.08371167\n\n\nHence the point estimate for the log-odds is 0.06, which has a corresponding 95% confidence interval of (0.04, 0.08). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model, show.values = TRUE, transform = NULL,\n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds of age for male instructors.\n\n\n\nSome of the interesting arguments that can be passed to the plot_model function are:\n\n\nshow.values = TRUE/FALSE: Whether the log-odds/odds values should be displayed;\n\nshow.p = TRUE/FALSE: Adds asterisks that indicate the significance level of estimates to the value labels;\n\ntransform: A character vector naming the function that will be applied to the estimates. The default transformation uses exp to display the odds ratios, while transform = NULL displays the log-odds; and\n\nvline.color: colour of the vertical “zero effect” line.\n\nFurther details on using plot_model can be found here. Now, let’s add the estimates of the log-odds to our data set:\n\nCodeevals.gender &lt;- evals.gender |&gt;\n                  mutate(logodds.male = predict(model))\n\n# A tibble: 6 × 3\n  gender   age logodds.male\n  &lt;fct&gt;  &lt;int&gt;        &lt;dbl&gt;\n1 female    36       -0.431\n2 female    36       -0.431\n3 female    36       -0.431\n4 female    36       -0.431\n5 male      59        1.02 \n6 male      59        1.02"
  },
  {
    "objectID": "index.html#odds",
    "href": "index.html#odds",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.3 Odds",
    "text": "2.3 Odds\nTypically we would like to work on the odds scale as it is easier to interpret an odds-ratio as opposed to the log-odds-ratio. To obtain the odds we simply exponentiate the log-odds, that is\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right), \\nonumber\n\\end{align}\\]\n\nCodemodel |&gt;\n coef() |&gt;\n  exp()\n\n(Intercept)         age \n 0.06734369  1.06498927 \n\n\nOn the odds scale, the value of the intercept (0.07) gives the odds of a teaching instructor being male given their age = 0, which is obviously not a viable age for a teaching instructor, and hence why this value is very close to zero. For age we have an odds of 1.06, which indicates that for every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of 1.06. So how is this calculated? Let’s look at the odds-ratio obtained from instructors aged 51 and 52 years old, that is, a one unit difference:\n\\[\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{age=52}}}{\\mbox{Odds}_{\\mbox{age=51}}} = \\left(\\frac{\\frac{p_{\\mbox{age=52}}}{1 - p_{\\mbox{age=52}}}}{\\frac{p_{\\mbox{age=51}}}{1 - p_{\\mbox{age=51}}}}\\right) = \\frac{\\exp\\left(\\alpha + \\beta \\cdot 52\\right)}{\\exp\\left(\\alpha + \\beta \\cdot 51\\right)} = \\exp\\left(\\beta \\cdot (52 - 51)\\right) = \\exp\\left(0.06\\right) = 1.06. \\nonumber\n\\end{align}\\]\nFor example, the odds of a teaching instructor who is 45 years old being male is given by\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age}\\right) = \\exp\\left(-2.7 + 0.06 \\cdot 45\\right) = 1.15. \\nonumber\n\\end{align}\\]\nThis can be interpreted as the chances of an instructor who is 45 being male are 15% greater than them being female. We can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of our log-odds interval:\n\nCodeage.odds.lower &lt;- exp(age.logodds.lower)\n\n[1] 1.043122\n\nCodeage.odds.upper &lt;- exp(age.logodds.upper)\n\n[1] 1.087315\n\n\nHence the point estimate for the odds is 1.06, which has a corresponding 95% confidence interval of (1.04, 1.09). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument as well as removing transform = NULL (the default transformation is exponential):\n\nCodeplot_model(model, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE, axis.lim = c(1, 1.5))\n\n\n\nThe odds of age for male instructors.\n\n\n\nNote: axis.lim is used to zoom in on the 95% confidence interval. The confint() function can also be used to compute confidence intervals (confint(model) for example).\nNow, let’s add the estimates of the odds to our data set:\n\nCodeevals.gender &lt;- evals.gender |&gt;\n                  mutate(odds.male = exp(logodds.male))\n\n# A tibble: 6 × 4\n  gender   age logodds.male odds.male\n  &lt;fct&gt;  &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 female    36       -0.431     0.650\n2 female    36       -0.431     0.650\n3 female    36       -0.431     0.650\n4 female    36       -0.431     0.650\n5 male      59        1.02      2.76 \n6 male      59        1.02      2.76"
  },
  {
    "objectID": "index.html#probabilities",
    "href": "index.html#probabilities",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.4 Probabilities",
    "text": "2.4 Probabilities\nWe can obtain the probability \\(p = \\textrm{Prob}(\\textrm{Male})\\) using the following transformation:\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}. \\nonumber\n\\end{align}\\]\nFor example, the probability of a teaching instructor who is 52 years old being male is\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}\n=\\frac{\\exp\\left(-2.697946 + 0.0629647\\cdot 52 \\right)}{1 + \\exp\\left(-2.697946 + 0.0629647\\cdot 52 \\right)}\n= 0.64, \\nonumber\n\\end{align}\\]\nwhich can be computed in R as follows:\n\nCodep.num &lt;- (exp(mod.coef.logodds[\"(Intercept)\", \"Estimate\"] \n              + mod.coef.logodds[\"age\", \"Estimate\"] * 52))\np.denom &lt;- 1 + p.num\np.num / p.denom\n\n[1] 0.6401971\n\n\nThe plogis() function from the stats library can also be used to obtain probabilities from the log-odds:\n\nCodeplogis(mod.coef.logodds[\"(Intercept)\", \"Estimate\"] \n        + mod.coef.logodds[\"age\", \"Estimate\"] * 52)\n\n[1] 0.6401971\n\n\nLet’s add the probabilities to our data, which is done using the fitted() function:\n\nCodeevals.gender &lt;- evals.gender |&gt;\n                  mutate(probs.male = fitted(model))\n\n\nNote: predict(model, type = \"response\") will also provide the estimated probabilities.\nFinally, we can plot the probability of being male using the geom_smooth() function by giving method = \"glm\" and methods.args = list(family = \"binomial\") as follows:\n\n\n\n\n\n\n\n\n\nCodeggplot(data = evals.gender, aes(x = age, y = probs.male)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  labs(x = \"Age\", y = \"Probability of instructor being male\")\n\n\n\nProbability of teaching instructor being male by age.\n\n\n\n\n\nThe plot_model() function from the sjPlot package can also produce the estimated probabilities by age as follows:\n\nCodeplot_model(model, type = \"pred\", title = \"\", terms=\"age [all]\", axis.title = c(\"Age\", \"Prob. of instructor being male\"))\n\n\n\nProbability of teaching instructor being male by age."
  },
  {
    "objectID": "index.html#log-odds-1",
    "href": "index.html#log-odds-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.1 Log-odds",
    "text": "3.1 Log-odds\nThe logistic regression model is given by:\n\nCodemodel.ethnic &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(gender ~ ethnicity, data = evals.ethnic) |&gt;\n  extract_fit_engine()\n\n\n\nCodemodel.ethnic |&gt;\n  summary()\n\n\nCall:\nstats::glm(formula = gender ~ ethnicity, family = stats::binomial, \n    data = data)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)            -0.2513     0.2520  -0.997   0.3186  \nethnicitynot minority   0.6630     0.2719   2.438   0.0148 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 630.30  on 462  degrees of freedom\nResidual deviance: 624.29  on 461  degrees of freedom\nAIC: 628.29\n\nNumber of Fisher Scoring iterations: 4\n\n\nAgain, the baseline category for our binary response is female. Also, the baseline category for our explanatory variable is minority, which, like gender, is done alphabetically by default by R:\n\nCodelevels(evals.ethnic$ethnicity)\n\n[1] \"minority\"     \"not minority\"\n\n\nThis means that estimates from the logistic regression model are for a change on the log-odds scale for males (\\(p = \\textrm{Prob}(\\textrm{Males})\\)) in comparison to the response baseline females. That is\n\\[\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\cdot \\textrm{ethnicity} = -0.25 + 0.66 \\cdot \\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority}), \\nonumber\n\\end{align}\\]\nwhere \\(\\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority})\\) is an indicator function. Hence, the log-odds of an instructor being male increase by 0.66 if they are in the ethnicity group not minority. This provides us with a point estimate of how the log-odds changes with ethnicity, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:\n\nCodemod.ethnic.coef.logodds &lt;- model.ethnic |&gt;\n                            summary() |&gt;\n                            coef()\n\n\n\nCodeethnic.logodds.lower &lt;- (mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"] \n                          - 1.96 *\n                           mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Std. Error\"])\n\n[1] 0.1300587\n\nCodeethnic.logodds.upper &lt;- (mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"] \n                          + 1.96 *\n                           mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Std. Error\"])\n\n[1] 1.19604\n\n\nHence the point estimate for the log-odds is 0.66, which has a corresponding 95% confidence interval of (0.13, 1.2). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model.ethnic, show.values = TRUE, transform = NULL, \n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds for male instructors by ethnicity (not a minority).\n\n\n\nNow, let’s add the estimates of the log-odds to our data set:\n\nCodeevals.ethnic &lt;- evals.ethnic |&gt;\n                  mutate(logodds.male = predict(model.ethnic))\n\n# A tibble: 6 × 3\n  gender ethnicity    logodds.male\n  &lt;fct&gt;  &lt;fct&gt;               &lt;dbl&gt;\n1 female minority           -0.251\n2 female minority           -0.251\n3 female minority           -0.251\n4 female minority           -0.251\n5 male   not minority        0.412\n6 male   not minority        0.412"
  },
  {
    "objectID": "index.html#odds-1",
    "href": "index.html#odds-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.2 Odds",
    "text": "3.2 Odds\nOn the odds scale the regression coefficients are given by\n\nCodemodel.ethnic |&gt;\n coef() |&gt;\n  exp()\n\n          (Intercept) ethnicitynot minority \n            0.7777778             1.9407008 \n\n\nThe (Intercept) gives us the odds of the instructor being male given that they are in the minority ethnic group, that is, 0.78 (the indicator function is zero in that case). The odds of the instructor being male given they are in the not minority ethnic group are 1.94 times greater than the odds if they were in the minority ethnic group.\nBefore moving on, let’s take a look at how these values are computed. First, the odds of the instructor being male given that they are in the minority ethnic group can be obtained as follows:\n\\[\\begin{align}\n\\frac{p_{\\mbox{minority}}}{1 - p_{\\mbox{minority}}} = \\exp\\left(\\alpha\\right) = \\exp\\left(-0.25\\right) = 0.78. \\nonumber\n\\end{align}\\]\n\nCode# the number of instructors in the minority\npmin &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"minority\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the number of male instructors in the minority\npmin.male &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"minority\", gender == \"male\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the proportion/probability of males in the minority\nprob.min.male &lt;- pmin.male / pmin\nodds.min.male &lt;- prob.min.male / (1 - prob.min.male)\nodds.min.male\n\n[1] 0.7777778\n\n\nNow, the odds-ratio of an instructor being male in the not minority compared to the minority ethnic group is found as follows:\n\\[\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{not minority}}}{\\mbox{Odds}_{\\mbox{minority}}} = \\frac{\\frac{p_{\\mbox{not minority}}}{1 - p_{\\mbox{not minority}}}}{\\frac{p_{\\mbox{minority}}}{1 - p_{\\mbox{minority}}}} &= \\frac{\\exp\\left(\\alpha + \\beta\\right)}{\\exp\\left(\\alpha\\right)} = \\exp\\left(\\alpha + \\beta - \\alpha\\right) = \\exp\\left(\\beta\\right) = \\exp\\left(0.66 \\right) = 1.93. \\nonumber\n\\end{align}\\]\n\nCode# the number of instructors not in the minority\npnotmin &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"not minority\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the number of male instructors not in the minority\npnotmin.male &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"not minority\", gender == \"male\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the proportion/probability of males not in the minority\nprob.notmin.male &lt;- pnotmin.male / pnotmin\nodds.notmin.male &lt;- prob.notmin.male / (1 - prob.notmin.male)\nodds.ratio.notmin &lt;- odds.notmin.male / odds.min.male\n\n[1] 1.940701\n\n\nWe can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of the log-odds interval:\n\nCodeethnic.odds.lower &lt;- exp(ethnic.logodds.lower)\n\n[1] 1.138895\n\nCodeethnic.odds.upper &lt;- exp(ethnic.logodds.upper)\n\n[1] 3.306994\n\n\nHence the point estimate for the odds-ratio is 1.94, which has a corresponding 95% confidence interval of (1.14, 3.31). Again, we can display this graphically using the plot_model function from the sjPlot package:\n\nCodeplot_model(model.ethnic, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe odds-ratio of a male instructor given they are in the not minority group.\n\n\n\nNow, let’s add the estimates of the odds to our data set:\n\nCodeevals.ethnic &lt;- evals.ethnic |&gt;\n                  mutate(odds.male = exp(logodds.male))\n\n# A tibble: 6 × 4\n  gender ethnicity    logodds.male odds.male\n  &lt;fct&gt;  &lt;fct&gt;               &lt;dbl&gt;     &lt;dbl&gt;\n1 female minority           -0.251     0.778\n2 female minority           -0.251     0.778\n3 female minority           -0.251     0.778\n4 female minority           -0.251     0.778\n5 male   not minority        0.412     1.51 \n6 male   not minority        0.412     1.51"
  },
  {
    "objectID": "index.html#probabilities-1",
    "href": "index.html#probabilities-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.3 Probabilities",
    "text": "3.3 Probabilities\nThe probabilities of an instructor being male given they are in the minority and not minority groups are\n\nCodeplogis(mod.ethnic.coef.logodds[\"(Intercept)\", \"Estimate\"])\n\n[1] 0.4375\n\nCodeplogis(mod.ethnic.coef.logodds[\"(Intercept)\", \"Estimate\"] +\n         mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"])\n\n[1] 0.6015038\n\n\nHence, the probabilities of an instructor being male given they are in the minority and not minority ethnic groups are 0.437 and 0.602, respectively.\nLet’s add the probabilities to our data:\n\nCodeevals.ethnic &lt;- evals.ethnic |&gt;\n                  mutate(probs.male = fitted(model.ethnic))\n\n\nFinally, we can use the plot_model() function from the sjPlot package to produce the estimated probabilities by ethnicity as follows:\n\nCodeplot_model(model.ethnic, type = \"pred\", terms = \"ethnicity\", axis.title = c(\"Ethnicity\", \"Prob. of instructor being male\"), title = \" \")\n\n\n\nProbability of teaching instructor being male by ethnicity."
  },
  {
    "objectID": "DA_Week7_FurtherTasksSolutions.html",
    "href": "DA_Week7_FurtherTasksSolutions.html",
    "title": "Data Analysis\nWeek 7 Further Tasks Solutions",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\nSolution\n\nyanny &lt;- read_csv(\"yanny.csv\")\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\nyanny$hear &lt;- as.factor(yanny$hear)\nyanny$gender &lt;- as.factor(yanny$gender)\n\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\n\nBarplot of what participants heard by gender.\n\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; \n  fit(hear ~ age + gender, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age + gender, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.62792    1.24392   1.309    0.191\nage         -0.04839    0.03404  -1.422    0.155\ngenderMale  -0.20637    0.56935  -0.362    0.717\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.454  on 49  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 75.454\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; fit(hear ~ age, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age, family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.51874    1.21032   1.255     0.21\nage         -0.04812    0.03423  -1.406     0.16\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.586  on 50  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 73.586\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.04812) = 0.953. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.953 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(0.04812 * 10) = 1.618. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 1.618 times those of the younger person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\n\nplot_model(mod.yanny, type = \"pred\", terms = \"age\", title = \"\", axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\n\nProbability of hearing Yanny with age.\n\n\n\n\n\n\n\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\nSolution\n\ntitanic &lt;- read_csv(\"titanic.csv\")\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class)\ntitanic$survived &lt;- as.factor(titanic$survived)\nlevels(titanic$survived) &lt;- c(\"Died\", \"Survived\")\ntitanic$gender &lt;- as.factor(titanic$gender)\ntitanic$passenger.class &lt;- as.factor(titanic$passenger.class)\n\n\nggplot(data = titanic, aes(x = survived, y = age, fill = survived)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nTitanic passenger age by survival.\n\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class        Died    Survived\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\n\nmod.titanic &lt;- logistic_reg()\nmod.titanic &lt;- mod.titanic |&gt; fit (survived ~ gender + passenger.class + age, data = titanic) |&gt;\n  extract_fit_engine()\nmod.titanic %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = survived ~ gender + passenger.class + age, \n    family = stats::binomial, data = data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.54474    0.36537   9.702  &lt; 2e-16 ***\ngendermale       -2.61131    0.18671 -13.986  &lt; 2e-16 ***\npassenger.class2 -1.12216    0.25773  -4.354 1.34e-05 ***\npassenger.class3 -2.32917    0.24089  -9.669  &lt; 2e-16 ***\nage              -0.03330    0.00737  -4.519 6.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  805.29  on 886  degrees of freedom\nAIC: 815.29\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic, show.values = TRUE,\n           title = \"\", show.p = FALSE, value.offset = 0.25)\n\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.07 times those of women, third class passengers’ odds of survival were 0.10 times those of first class passengers, and second class passengers’ odds of survival were 0.33 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.97)."
  },
  {
    "objectID": "DA_Week7_FurtherTasksSolutions.html#yanny-or-laurel",
    "href": "DA_Week7_FurtherTasksSolutions.html#yanny-or-laurel",
    "title": "Data Analysis\nWeek 7 Further Tasks Solutions",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\nSolution\n\nyanny &lt;- read_csv(\"yanny.csv\")\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\nyanny$hear &lt;- as.factor(yanny$hear)\nyanny$gender &lt;- as.factor(yanny$gender)\n\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\n\nBarplot of what participants heard by gender.\n\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; \n  fit(hear ~ age + gender, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age + gender, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.62792    1.24392   1.309    0.191\nage         -0.04839    0.03404  -1.422    0.155\ngenderMale  -0.20637    0.56935  -0.362    0.717\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.454  on 49  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 75.454\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; fit(hear ~ age, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age, family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.51874    1.21032   1.255     0.21\nage         -0.04812    0.03423  -1.406     0.16\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.586  on 50  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 73.586\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.04812) = 0.953. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.953 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(0.04812 * 10) = 1.618. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 1.618 times those of the younger person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\n\nplot_model(mod.yanny, type = \"pred\", terms = \"age\", title = \"\", axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\n\nProbability of hearing Yanny with age."
  },
  {
    "objectID": "DA_Week7_FurtherTasksSolutions.html#titanic",
    "href": "DA_Week7_FurtherTasksSolutions.html#titanic",
    "title": "Data Analysis\nWeek 7 Further Tasks Solutions",
    "section": "",
    "text": "On 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\nSolution\n\ntitanic &lt;- read_csv(\"titanic.csv\")\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class)\ntitanic$survived &lt;- as.factor(titanic$survived)\nlevels(titanic$survived) &lt;- c(\"Died\", \"Survived\")\ntitanic$gender &lt;- as.factor(titanic$gender)\ntitanic$passenger.class &lt;- as.factor(titanic$passenger.class)\n\n\nggplot(data = titanic, aes(x = survived, y = age, fill = survived)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nTitanic passenger age by survival.\n\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class        Died    Survived\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\n\nmod.titanic &lt;- logistic_reg()\nmod.titanic &lt;- mod.titanic |&gt; fit (survived ~ gender + passenger.class + age, data = titanic) |&gt;\n  extract_fit_engine()\nmod.titanic %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = survived ~ gender + passenger.class + age, \n    family = stats::binomial, data = data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.54474    0.36537   9.702  &lt; 2e-16 ***\ngendermale       -2.61131    0.18671 -13.986  &lt; 2e-16 ***\npassenger.class2 -1.12216    0.25773  -4.354 1.34e-05 ***\npassenger.class3 -2.32917    0.24089  -9.669  &lt; 2e-16 ***\nage              -0.03330    0.00737  -4.519 6.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  805.29  on 886  degrees of freedom\nAIC: 815.29\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic, show.values = TRUE,\n           title = \"\", show.p = FALSE, value.offset = 0.25)\n\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.07 times those of women, third class passengers’ odds of survival were 0.10 times those of first class passengers, and second class passengers’ odds of survival were 0.33 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.97)."
  },
  {
    "objectID": "solutions/Week7_Solutions.html",
    "href": "solutions/Week7_Solutions.html",
    "title": "Week 7 Tasks Solutions",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\nSolution\n\nyanny &lt;- read_csv(\"yanny.csv\")\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\nyanny$hear &lt;- as.factor(yanny$hear)\nyanny$gender &lt;- as.factor(yanny$gender)\n\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\n\nBarplot of what participants heard by gender.\n\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; \n  fit(hear ~ age + gender, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age + gender, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.62792    1.24392   1.309    0.191\nage         -0.04839    0.03404  -1.422    0.155\ngenderMale  -0.20637    0.56935  -0.362    0.717\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.454  on 49  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 75.454\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; fit(hear ~ age, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age, family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.51874    1.21032   1.255     0.21\nage         -0.04812    0.03423  -1.406     0.16\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.586  on 50  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 73.586\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.04812) = 0.953. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.953 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(0.04812 * 10) = 1.618. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 1.618 times those of the younger person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\n\nplot_model(mod.yanny, type = \"pred\", terms = \"age\", title = \"\", axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\n\nProbability of hearing Yanny with age.\n\n\n\n\n\n\n\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\nSolution\n\ntitanic &lt;- read_csv(\"titanic.csv\")\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class)\ntitanic$survived &lt;- as.factor(titanic$survived)\nlevels(titanic$survived) &lt;- c(\"Died\", \"Survived\")\ntitanic$gender &lt;- as.factor(titanic$gender)\ntitanic$passenger.class &lt;- as.factor(titanic$passenger.class)\n\n\nggplot(data = titanic, aes(x = survived, y = age, fill = survived)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nTitanic passenger age by survival.\n\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class        Died    Survived\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\n\nmod.titanic &lt;- logistic_reg()\nmod.titanic &lt;- mod.titanic |&gt; fit (survived ~ gender + passenger.class + age, data = titanic) |&gt;\n  extract_fit_engine()\nmod.titanic %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = survived ~ gender + passenger.class + age, \n    family = stats::binomial, data = data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.54474    0.36537   9.702  &lt; 2e-16 ***\ngendermale       -2.61131    0.18671 -13.986  &lt; 2e-16 ***\npassenger.class2 -1.12216    0.25773  -4.354 1.34e-05 ***\npassenger.class3 -2.32917    0.24089  -9.669  &lt; 2e-16 ***\nage              -0.03330    0.00737  -4.519 6.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  805.29  on 886  degrees of freedom\nAIC: 815.29\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic, show.values = TRUE,\n           title = \"\", show.p = FALSE, value.offset = 0.25)\n\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.07 times those of women, third class passengers’ odds of survival were 0.10 times those of first class passengers, and second class passengers’ odds of survival were 0.33 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.97)."
  },
  {
    "objectID": "solutions/Week7_Solutions.html#yanny-or-laurel",
    "href": "solutions/Week7_Solutions.html#yanny-or-laurel",
    "title": "Week 7 Tasks Solutions",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\nSolution\n\nyanny &lt;- read_csv(\"yanny.csv\")\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\nyanny$hear &lt;- as.factor(yanny$hear)\nyanny$gender &lt;- as.factor(yanny$gender)\n\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\n\nBarplot of what participants heard by gender.\n\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; \n  fit(hear ~ age + gender, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age + gender, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.62792    1.24392   1.309    0.191\nage         -0.04839    0.03404  -1.422    0.155\ngenderMale  -0.20637    0.56935  -0.362    0.717\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.454  on 49  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 75.454\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; fit(hear ~ age, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age, family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.51874    1.21032   1.255     0.21\nage         -0.04812    0.03423  -1.406     0.16\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.586  on 50  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 73.586\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.04812) = 0.953. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.953 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(0.04812 * 10) = 1.618. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 1.618 times those of the younger person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\n\nplot_model(mod.yanny, type = \"pred\", terms = \"age\", title = \"\", axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\n\nProbability of hearing Yanny with age."
  },
  {
    "objectID": "solutions/Week7_Solutions.html#titanic",
    "href": "solutions/Week7_Solutions.html#titanic",
    "title": "Week 7 Tasks Solutions",
    "section": "",
    "text": "On 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\nSolution\n\ntitanic &lt;- read_csv(\"titanic.csv\")\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class)\ntitanic$survived &lt;- as.factor(titanic$survived)\nlevels(titanic$survived) &lt;- c(\"Died\", \"Survived\")\ntitanic$gender &lt;- as.factor(titanic$gender)\ntitanic$passenger.class &lt;- as.factor(titanic$passenger.class)\n\n\nggplot(data = titanic, aes(x = survived, y = age, fill = survived)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nTitanic passenger age by survival.\n\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class        Died    Survived\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\n\nmod.titanic &lt;- logistic_reg()\nmod.titanic &lt;- mod.titanic |&gt; fit (survived ~ gender + passenger.class + age, data = titanic) |&gt;\n  extract_fit_engine()\nmod.titanic %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = survived ~ gender + passenger.class + age, \n    family = stats::binomial, data = data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.54474    0.36537   9.702  &lt; 2e-16 ***\ngendermale       -2.61131    0.18671 -13.986  &lt; 2e-16 ***\npassenger.class2 -1.12216    0.25773  -4.354 1.34e-05 ***\npassenger.class3 -2.32917    0.24089  -9.669  &lt; 2e-16 ***\nage              -0.03330    0.00737  -4.519 6.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  805.29  on 886  degrees of freedom\nAIC: 815.29\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic, show.values = TRUE,\n           title = \"\", show.p = FALSE, value.offset = 0.25)\n\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.07 times those of women, third class passengers’ odds of survival were 0.10 times those of first class passengers, and second class passengers’ odds of survival were 0.33 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.97)."
  },
  {
    "objectID": "Week_7_0/about.html",
    "href": "Week_7_0/about.html",
    "title": "Week 7 Tasks",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\n\n\n\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?"
  },
  {
    "objectID": "Week_7_0/about.html#yanny-or-laurel",
    "href": "Week_7_0/about.html#yanny-or-laurel",
    "title": "Week 7 Tasks",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?"
  },
  {
    "objectID": "Week_7_0/about.html#titanic",
    "href": "Week_7_0/about.html#titanic",
    "title": "Week 7 Tasks",
    "section": "",
    "text": "On 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?"
  },
  {
    "objectID": "Week_7_0/index.html",
    "href": "Week_7_0/index.html",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "In Weeks 3 and 4 we looked at modelling data using linear regression models were we had:\n\na continuous response variable \\(y\\) and\none or more explanatory variables \\(x_1, x_2,\\ldots, x_p\\), which were numerical/categorical variables.\n\nRecall that for data \\((y_i, x_i), ~ i = 1,\\ldots, n\\), where \\(y\\) is a continuous response variable, we can write a simple linear regression model as follows:\n\\[y_i = \\alpha + \\beta x_i + \\epsilon_i, ~~~~ \\epsilon_i \\sim N(0, \\sigma^2),\\] where\n\n\n\\(y_i\\) is the \\(i^{th}\\) observation of the continuous response variable;\n\n\\(\\alpha\\) is the intercept of the regression line;\n\n\\(\\beta\\) is the slope of the regression line;\n\n\\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and\n\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random component.\n\nThus, the full probability model for \\(y_i\\) given \\(x_i\\) (\\(y_i | x_i\\)) can be written as\n\\[y_i | x_i \\sim N(\\alpha + \\beta x_i, \\sigma^2),\\]\nwhere the mean \\(\\alpha + \\beta x_i\\) is given by the deterministic part of the model and the variance \\(\\sigma^2\\) by the random part. Hence we make the assumption that the outcomes \\(y_i\\) are normally distributed with mean \\(\\alpha + \\beta x_i\\) and variance \\(\\sigma^2\\). However, what if our response variable \\(y\\) is not a continuous random variable?\n\nThe main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nWhat if our response variable \\(y\\) is binary (e.g. yes/no, success/failure, alive/dead)? That is, the independent responses \\(y_i\\) can either be:\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\nbinomial, where \\(y_i\\) is the number of successes in a given number of trials \\(n_i\\), with the probability of success being \\(p_i\\) and the probability of failure being \\(1-p_i\\).\n\nIn both cases the distribution of \\(y_i\\) is assumed to be binomial, but in the first case it is Bin\\((1,p_i)\\) and in the second case it is Bin\\((n_i,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) equal to the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\] which is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\] This linear regression model with a binary response variable is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)\n\n\n\nBefore we proceed, load all the packages needed for this week:\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(gapminder)\nlibrary(sjPlot)\nlibrary(stats)\nlibrary(readr)\nlibrary(janitor)\nlibrary(tidymodels)"
  },
  {
    "objectID": "Week_7_0/index.html#generalised-linear-models",
    "href": "Week_7_0/index.html#generalised-linear-models",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "The main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nWhat if our response variable \\(y\\) is binary (e.g. yes/no, success/failure, alive/dead)? That is, the independent responses \\(y_i\\) can either be:\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\nbinomial, where \\(y_i\\) is the number of successes in a given number of trials \\(n_i\\), with the probability of success being \\(p_i\\) and the probability of failure being \\(1-p_i\\).\n\nIn both cases the distribution of \\(y_i\\) is assumed to be binomial, but in the first case it is Bin\\((1,p_i)\\) and in the second case it is Bin\\((n_i,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) equal to the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\] which is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\] This linear regression model with a binary response variable is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)"
  },
  {
    "objectID": "Week_7_0/index.html#required-r-packages",
    "href": "Week_7_0/index.html#required-r-packages",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "Before we proceed, load all the packages needed for this week:\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(gapminder)\nlibrary(sjPlot)\nlibrary(stats)\nlibrary(readr)\nlibrary(janitor)\nlibrary(tidymodels)"
  },
  {
    "objectID": "Week_7_0/index.html#teaching-evaluation-scores",
    "href": "Week_7_0/index.html#teaching-evaluation-scores",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.1 Teaching evaluation scores",
    "text": "2.1 Teaching evaluation scores\nStudent feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see Economics of Education Review for details. Here, we shall look at a study from student evaluations of \\(n=463\\) professors from The University of Texas at Austin.\nPreviously, we looked at teaching score as our continuous response variable and beauty score as our explanatory variable. Now we shall consider gender as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in gender by age of the teaching instructors within the evals data set.\nFirst, let’s start by selecting the variables of interest from the evals data set:\n\nCodeevals.gender &lt;- evals |&gt;\n                  select(gender, age)\n\n# A tibble: 6 × 2\n  gender   age\n  &lt;fct&gt;  &lt;int&gt;\n1 female    36\n2 female    36\n3 female    36\n4 female    36\n5 male      59\n6 male      59\n\n\nNow, let’s look at a boxplot of age by gender to get an initial impression of the data:\n\nCodeggplot(data = evals.gender, aes(x = gender, y = age, fill = gender)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\nTeaching instructor age by gender.\n\n\n\nHere we can see that the age of male teaching instructors tends to be higher than that of their female counterparts. Now, let’s fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female."
  },
  {
    "objectID": "Week_7_0/index.html#log-odds",
    "href": "Week_7_0/index.html#log-odds",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.2 Log-odds",
    "text": "2.2 Log-odds\nTo fit a logistic regression model we will use the generalised linear model function glm, which acts in a very similar manner to the lm function we have used previously. We only have to deal with an additional argument. The logistic regression model with gender as the response and age as the explanatory variable is given by:\n\nCodemodel &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\nmodel &lt;- model |&gt; \n  fit(gender ~ age, data = evals.gender) |&gt;\n  extract_fit_engine()\n\n\nHere we include the additional family argument, which states the distribution and link function we would like to use. Hence family = binomial(link = \"logit\") states we have a binary response variable, and thus have a binomial distribution, with its corresponding logit link function. Now, let’s take a look at the summary produced from our logistic regression model:\n\nCodemodel |&gt;\n  summary()\n\n\nCall:\nstats::glm(formula = gender ~ age, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.69795    0.51194  -5.270 1.36e-07 ***\nage          0.06296    0.01059   5.948 2.71e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 630.30  on 462  degrees of freedom\nResidual deviance: 591.41  on 461  degrees of freedom\nAIC: 595.41\n\nNumber of Fisher Scoring iterations: 4\n\n\nFirstly, the baseline category for our binary response is female. This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the levels function:\n\nCodelevels(evals.gender$gender)\n\n[1] \"female\" \"male\"  \n\n\nThis means that estimates from the logistic regression model are for a change on the log-odds scale for males in comparison to the response baseline females. That is\n\\[\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\cdot \\textrm{age} = -2.7 + 0.06 \\cdot \\textrm{age}, \\nonumber\n\\end{align}\\]\nwhere \\(p = \\textrm{Prob}\\left(\\textrm{Male}\\right)\\) and \\(1 - p = \\textrm{Prob}\\left(\\textrm{Female}\\right)\\). Hence, the log-odds of the instructor being male increase by 0.06 for every one unit increase in age. This provides us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:\n\nCodemod.coef.logodds &lt;- model |&gt;\n                      summary() |&gt;\n                      coef()\n\n\n\nCodeage.logodds.lower &lt;- (mod.coef.logodds[\"age\", \"Estimate\"] \n                        - 1.96 * mod.coef.logodds[\"age\", \"Std. Error\"])\n\n[1] 0.04221777\n\nCodeage.logodds.upper &lt;- (mod.coef.logodds[\"age\", \"Estimate\"] \n                        + 1.96 * mod.coef.logodds[\"age\", \"Std. Error\"])\n\n[1] 0.08371167\n\n\nHence the point estimate for the log-odds is 0.06, which has a corresponding 95% confidence interval of (0.04, 0.08). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model, show.values = TRUE, transform = NULL,\n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds of age for male instructors.\n\n\n\nSome of the interesting arguments that can be passed to the plot_model function are:\n\n\nshow.values = TRUE/FALSE: Whether the log-odds/odds values should be displayed;\n\nshow.p = TRUE/FALSE: Adds asterisks that indicate the significance level of estimates to the value labels;\n\ntransform: A character vector naming the function that will be applied to the estimates. The default transformation uses exp to display the odds ratios, while transform = NULL displays the log-odds; and\n\nvline.color: colour of the vertical “zero effect” line.\n\nFurther details on using plot_model can be found here. Now, let’s add the estimates of the log-odds to our data set:\n\nCodeevals.gender &lt;- evals.gender |&gt;\n                  mutate(logodds.male = predict(model))\n\n# A tibble: 6 × 3\n  gender   age logodds.male\n  &lt;fct&gt;  &lt;int&gt;        &lt;dbl&gt;\n1 female    36       -0.431\n2 female    36       -0.431\n3 female    36       -0.431\n4 female    36       -0.431\n5 male      59        1.02 \n6 male      59        1.02"
  },
  {
    "objectID": "Week_7_0/index.html#odds",
    "href": "Week_7_0/index.html#odds",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.3 Odds",
    "text": "2.3 Odds\nTypically we would like to work on the odds scale as it is easier to interpret an odds-ratio as opposed to the log-odds-ratio. To obtain the odds we simply exponentiate the log-odds, that is\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right), \\nonumber\n\\end{align}\\]\n\nCodemodel |&gt;\n coef() |&gt;\n  exp()\n\n(Intercept)         age \n 0.06734369  1.06498927 \n\n\nOn the odds scale, the value of the intercept (0.07) gives the odds of a teaching instructor being male given their age = 0, which is obviously not a viable age for a teaching instructor, and hence why this value is very close to zero. For age we have an odds of 1.06, which indicates that for every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of 1.06. So how is this calculated? Let’s look at the odds-ratio obtained from instructors aged 51 and 52 years old, that is, a one unit difference:\n\\[\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{age=52}}}{\\mbox{Odds}_{\\mbox{age=51}}} = \\left(\\frac{\\frac{p_{\\mbox{age=52}}}{1 - p_{\\mbox{age=52}}}}{\\frac{p_{\\mbox{age=51}}}{1 - p_{\\mbox{age=51}}}}\\right) = \\frac{\\exp\\left(\\alpha + \\beta \\cdot 52\\right)}{\\exp\\left(\\alpha + \\beta \\cdot 51\\right)} = \\exp\\left(\\beta \\cdot (52 - 51)\\right) = \\exp\\left(0.06\\right) = 1.06. \\nonumber\n\\end{align}\\]\nFor example, the odds of a teaching instructor who is 45 years old being male is given by\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age}\\right) = \\exp\\left(-2.7 + 0.06 \\cdot 45\\right) = 1.15. \\nonumber\n\\end{align}\\]\nThis can be interpreted as the chances of an instructor who is 45 being male are 15% greater than them being female. We can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of our log-odds interval:\n\nCodeage.odds.lower &lt;- exp(age.logodds.lower)\n\n[1] 1.043122\n\nCodeage.odds.upper &lt;- exp(age.logodds.upper)\n\n[1] 1.087315\n\n\nHence the point estimate for the odds is 1.06, which has a corresponding 95% confidence interval of (1.04, 1.09). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument as well as removing transform = NULL (the default transformation is exponential):\n\nCodeplot_model(model, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE, axis.lim = c(1, 1.5))\n\n\n\nThe odds of age for male instructors.\n\n\n\nNote: axis.lim is used to zoom in on the 95% confidence interval. The confint() function can also be used to compute confidence intervals (confint(model) for example).\nNow, let’s add the estimates of the odds to our data set:\n\nCodeevals.gender &lt;- evals.gender |&gt;\n                  mutate(odds.male = exp(logodds.male))\n\n# A tibble: 6 × 4\n  gender   age logodds.male odds.male\n  &lt;fct&gt;  &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 female    36       -0.431     0.650\n2 female    36       -0.431     0.650\n3 female    36       -0.431     0.650\n4 female    36       -0.431     0.650\n5 male      59        1.02      2.76 \n6 male      59        1.02      2.76"
  },
  {
    "objectID": "Week_7_0/index.html#probabilities",
    "href": "Week_7_0/index.html#probabilities",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.4 Probabilities",
    "text": "2.4 Probabilities\nWe can obtain the probability \\(p = \\textrm{Prob}(\\textrm{Male})\\) using the following transformation:\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}. \\nonumber\n\\end{align}\\]\nFor example, the probability of a teaching instructor who is 52 years old being male is\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}\n=\\frac{\\exp\\left(-2.697946 + 0.0629647\\cdot 52 \\right)}{1 + \\exp\\left(-2.697946 + 0.0629647\\cdot 52 \\right)}\n= 0.64, \\nonumber\n\\end{align}\\]\nwhich can be computed in R as follows:\n\nCodep.num &lt;- (exp(mod.coef.logodds[\"(Intercept)\", \"Estimate\"] \n              + mod.coef.logodds[\"age\", \"Estimate\"] * 52))\np.denom &lt;- 1 + p.num\np.num / p.denom\n\n[1] 0.6401971\n\n\nThe plogis() function from the stats library can also be used to obtain probabilities from the log-odds:\n\nCodeplogis(mod.coef.logodds[\"(Intercept)\", \"Estimate\"] \n        + mod.coef.logodds[\"age\", \"Estimate\"] * 52)\n\n[1] 0.6401971\n\n\nLet’s add the probabilities to our data, which is done using the fitted() function:\n\nCodeevals.gender &lt;- evals.gender |&gt;\n                  mutate(probs.male = fitted(model))\n\n\nNote: predict(model, type = \"response\") will also provide the estimated probabilities.\nFinally, we can plot the probability of being male using the geom_smooth() function by giving method = \"glm\" and methods.args = list(family = \"binomial\") as follows:\n\n\n\n\n\n\n\n\n\nCodeggplot(data = evals.gender, aes(x = age, y = probs.male)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  labs(x = \"Age\", y = \"Probability of instructor being male\")\n\n\n\nProbability of teaching instructor being male by age.\n\n\n\n\n\nThe plot_model() function from the sjPlot package can also produce the estimated probabilities by age as follows:\n\nCodeplot_model(model, type = \"pred\", title = \"\", terms=\"age [all]\", axis.title = c(\"Age\", \"Prob. of instructor being male\"))\n\n\n\nProbability of teaching instructor being male by age."
  },
  {
    "objectID": "Week_7_0/index.html#log-odds-1",
    "href": "Week_7_0/index.html#log-odds-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.1 Log-odds",
    "text": "3.1 Log-odds\nThe logistic regression model is given by:\n\nCodemodel.ethnic &lt;- glm(gender ~ ethnicity, data = evals.ethnic,\n                      family = binomial(link = \"logit\"))\n\n\n\nCodemodel.ethnic |&gt;\n  summary()\n\n\nCall:\nglm(formula = gender ~ ethnicity, family = binomial(link = \"logit\"), \n    data = evals.ethnic)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)            -0.2513     0.2520  -0.997   0.3186  \nethnicitynot minority   0.6630     0.2719   2.438   0.0148 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 630.30  on 462  degrees of freedom\nResidual deviance: 624.29  on 461  degrees of freedom\nAIC: 628.29\n\nNumber of Fisher Scoring iterations: 4\n\n\nAgain, the baseline category for our binary response is female. Also, the baseline category for our explanatory variable is minority, which, like gender, is done alphabetically by default by R:\n\nCodelevels(evals.ethnic$ethnicity)\n\n[1] \"minority\"     \"not minority\"\n\n\nThis means that estimates from the logistic regression model are for a change on the log-odds scale for males (\\(p = \\textrm{Prob}(\\textrm{Males})\\)) in comparison to the response baseline females. That is\n\\[\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\cdot \\textrm{ethnicity} = -0.25 + 0.66 \\cdot \\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority}), \\nonumber\n\\end{align}\\]\nwhere \\(\\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority})\\) is an indicator function. Hence, the log-odds of an instructor being male increase by 0.66 if they are in the ethnicity group not minority. This provides us with a point estimate of how the log-odds changes with ethnicity, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:\n\nCodemod.ethnic.coef.logodds &lt;- model.ethnic |&gt;\n                            summary() |&gt;\n                            coef()\n\n\n\nCodeethnic.logodds.lower &lt;- (mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"] \n                          - 1.96 *\n                           mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Std. Error\"])\n\n[1] 0.1300587\n\nCodeethnic.logodds.upper &lt;- (mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"] \n                          + 1.96 *\n                           mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Std. Error\"])\n\n[1] 1.19604\n\n\nHence the point estimate for the log-odds is 0.66, which has a corresponding 95% confidence interval of (0.13, 1.2). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model.ethnic, show.values = TRUE, transform = NULL, \n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds for male instructors by ethnicity (not a minority).\n\n\n\nNow, let’s add the estimates of the log-odds to our data set:\n\nCodeevals.ethnic &lt;- evals.ethnic |&gt;\n                  mutate(logodds.male = predict(model.ethnic))\n\n# A tibble: 6 × 3\n  gender ethnicity    logodds.male\n  &lt;fct&gt;  &lt;fct&gt;               &lt;dbl&gt;\n1 female minority           -0.251\n2 female minority           -0.251\n3 female minority           -0.251\n4 female minority           -0.251\n5 male   not minority        0.412\n6 male   not minority        0.412"
  },
  {
    "objectID": "Week_7_0/index.html#odds-1",
    "href": "Week_7_0/index.html#odds-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.2 Odds",
    "text": "3.2 Odds\nOn the odds scale the regression coefficients are given by\n\nCodemodel.ethnic |&gt;\n coef() |&gt;\n  exp()\n\n          (Intercept) ethnicitynot minority \n            0.7777778             1.9407008 \n\n\nThe (Intercept) gives us the odds of the instructor being male given that they are in the minority ethnic group, that is, 0.78 (the indicator function is zero in that case). The odds of the instructor being male given they are in the not minority ethnic group are 1.94 times greater than the odds if they were in the minority ethnic group.\nBefore moving on, let’s take a look at how these values are computed. First, the odds of the instructor being male given that they are in the minority ethnic group can be obtained as follows:\n\\[\\begin{align}\n\\frac{p_{\\mbox{minority}}}{1 - p_{\\mbox{minority}}} = \\exp\\left(\\alpha\\right) = \\exp\\left(-0.25\\right) = 0.78. \\nonumber\n\\end{align}\\]\n\nCode# the number of instructors in the minority\npmin &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"minority\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the number of male instructors in the minority\npmin.male &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"minority\", gender == \"male\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the proportion/probability of males in the minority\nprob.min.male &lt;- pmin.male / pmin\nodds.min.male &lt;- prob.min.male / (1 - prob.min.male)\nodds.min.male\n\n[1] 0.7777778\n\n\nNow, the odds-ratio of an instructor being male in the not minority compared to the minority ethnic group is found as follows:\n\\[\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{not minority}}}{\\mbox{Odds}_{\\mbox{minority}}} = \\frac{\\frac{p_{\\mbox{not minority}}}{1 - p_{\\mbox{not minority}}}}{\\frac{p_{\\mbox{minority}}}{1 - p_{\\mbox{minority}}}} &= \\frac{\\exp\\left(\\alpha + \\beta\\right)}{\\exp\\left(\\alpha\\right)} = \\exp\\left(\\alpha + \\beta - \\alpha\\right) = \\exp\\left(\\beta\\right) = \\exp\\left(0.66 \\right) = 1.93. \\nonumber\n\\end{align}\\]\n\nCode# the number of instructors not in the minority\npnotmin &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"not minority\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the number of male instructors not in the minority\npnotmin.male &lt;- evals.ethnic |&gt;\n              filter(ethnicity == \"not minority\", gender == \"male\") |&gt;\n              summarize(n()) |&gt;\n              pull()\n\n# the proportion/probability of males not in the minority\nprob.notmin.male &lt;- pnotmin.male / pnotmin\nodds.notmin.male &lt;- prob.notmin.male / (1 - prob.notmin.male)\nodds.ratio.notmin &lt;- odds.notmin.male / odds.min.male\n\n[1] 1.940701\n\n\nWe can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of the log-odds interval:\n\nCodeethnic.odds.lower &lt;- exp(ethnic.logodds.lower)\n\n[1] 1.138895\n\nCodeethnic.odds.upper &lt;- exp(ethnic.logodds.upper)\n\n[1] 3.306994\n\n\nHence the point estimate for the odds-ratio is 1.94, which has a corresponding 95% confidence interval of (1.14, 3.31). Again, we can display this graphically using the plot_model function from the sjPlot package:\n\nCodeplot_model(model.ethnic, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe odds-ratio of a male instructor given they are in the not minority group.\n\n\n\nNow, let’s add the estimates of the odds to our data set:\n\nCodeevals.ethnic &lt;- evals.ethnic |&gt;\n                  mutate(odds.male = exp(logodds.male))\n\n# A tibble: 6 × 4\n  gender ethnicity    logodds.male odds.male\n  &lt;fct&gt;  &lt;fct&gt;               &lt;dbl&gt;     &lt;dbl&gt;\n1 female minority           -0.251     0.778\n2 female minority           -0.251     0.778\n3 female minority           -0.251     0.778\n4 female minority           -0.251     0.778\n5 male   not minority        0.412     1.51 \n6 male   not minority        0.412     1.51"
  },
  {
    "objectID": "Week_7_0/index.html#probabilities-1",
    "href": "Week_7_0/index.html#probabilities-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.3 Probabilities",
    "text": "3.3 Probabilities\nThe probabilities of an instructor being male given they are in the minority and not minority groups are\n\nCodeplogis(mod.ethnic.coef.logodds[\"(Intercept)\", \"Estimate\"])\n\n[1] 0.4375\n\nCodeplogis(mod.ethnic.coef.logodds[\"(Intercept)\", \"Estimate\"] +\n         mod.ethnic.coef.logodds[\"ethnicitynot minority\", \"Estimate\"])\n\n[1] 0.6015038\n\n\nHence, the probabilities of an instructor being male given they are in the minority and not minority ethnic groups are 0.437 and 0.602, respectively.\nLet’s add the probabilities to our data:\n\nCodeevals.ethnic &lt;- evals.ethnic |&gt;\n                  mutate(probs.male = fitted(model.ethnic))\n\n\nFinally, we can use the plot_model() function from the sjPlot package to produce the estimated probabilities by ethnicity as follows:\n\nCodeplot_model(model.ethnic, type = \"pred\", terms = \"ethnicity\", axis.title = c(\"Ethnicity\", \"Prob. of instructor being male\"), title = \" \")\n\n\n\nProbability of teaching instructor being male by ethnicity."
  },
  {
    "objectID": "Week_7_0/Week7_Solutions.html",
    "href": "Week_7_0/Week7_Solutions.html",
    "title": "Week 7 Tasks Solutions",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\nSolution\n\nyanny &lt;- read_csv(\"yanny.csv\")\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\nyanny$hear &lt;- as.factor(yanny$hear)\nyanny$gender &lt;- as.factor(yanny$gender)\n\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\n\nBarplot of what participants heard by gender.\n\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; \n  fit(hear ~ age + gender, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age + gender, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.62792    1.24392   1.309    0.191\nage         -0.04839    0.03404  -1.422    0.155\ngenderMale  -0.20637    0.56935  -0.362    0.717\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.454  on 49  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 75.454\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; fit(hear ~ age, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age, family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.51874    1.21032   1.255     0.21\nage         -0.04812    0.03423  -1.406     0.16\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.586  on 50  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 73.586\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.04812) = 0.953. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.953 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(0.04812 * 10) = 1.618. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 1.618 times those of the younger person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\n\nplot_model(mod.yanny, type = \"pred\", terms = \"age\", title = \"\", axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\n\nProbability of hearing Yanny with age.\n\n\n\n\n\n\n\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\nSolution\n\ntitanic &lt;- read_csv(\"titanic.csv\")\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class)\ntitanic$survived &lt;- as.factor(titanic$survived)\nlevels(titanic$survived) &lt;- c(\"Died\", \"Survived\")\ntitanic$gender &lt;- as.factor(titanic$gender)\ntitanic$passenger.class &lt;- as.factor(titanic$passenger.class)\n\n\nggplot(data = titanic, aes(x = survived, y = age, fill = survived)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nTitanic passenger age by survival.\n\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class        Died    Survived\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\n\nmod.titanic &lt;- logistic_reg()\nmod.titanic &lt;- mod.titanic |&gt; fit (survived ~ gender + passenger.class + age, data = titanic) |&gt;\n  extract_fit_engine()\nmod.titanic %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = survived ~ gender + passenger.class + age, \n    family = stats::binomial, data = data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.54474    0.36537   9.702  &lt; 2e-16 ***\ngendermale       -2.61131    0.18671 -13.986  &lt; 2e-16 ***\npassenger.class2 -1.12216    0.25773  -4.354 1.34e-05 ***\npassenger.class3 -2.32917    0.24089  -9.669  &lt; 2e-16 ***\nage              -0.03330    0.00737  -4.519 6.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  805.29  on 886  degrees of freedom\nAIC: 815.29\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic, show.values = TRUE,\n           title = \"\", show.p = FALSE, value.offset = 0.25)\n\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.07 times those of women, third class passengers’ odds of survival were 0.10 times those of first class passengers, and second class passengers’ odds of survival were 0.33 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.97)."
  },
  {
    "objectID": "Week_7_0/Week7_Solutions.html#yanny-or-laurel",
    "href": "Week_7_0/Week7_Solutions.html#yanny-or-laurel",
    "title": "Week 7 Tasks Solutions",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\nSolution\n\nyanny &lt;- read_csv(\"yanny.csv\")\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\nyanny$hear &lt;- as.factor(yanny$hear)\nyanny$gender &lt;- as.factor(yanny$gender)\n\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\n\nBarplot of what participants heard by gender.\n\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; \n  fit(hear ~ age + gender, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age + gender, family = stats::binomial, \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.62792    1.24392   1.309    0.191\nage         -0.04839    0.03404  -1.422    0.155\ngenderMale  -0.20637    0.56935  -0.362    0.717\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.454  on 49  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 75.454\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nmod.yanny &lt;- logistic_reg()\nmod.yanny &lt;- mod.yanny |&gt; fit(hear ~ age, data = yanny) |&gt;\n  extract_fit_engine()\n\nmod.yanny %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = hear ~ age, family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  1.51874    1.21032   1.255     0.21\nage         -0.04812    0.03423  -1.406     0.16\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.779  on 51  degrees of freedom\nResidual deviance: 69.586  on 50  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 73.586\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.04812) = 0.953. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.953 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(0.04812 * 10) = 1.618. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 1.618 times those of the younger person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\n\nplot_model(mod.yanny, type = \"pred\", terms = \"age\", title = \"\", axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\n\nProbability of hearing Yanny with age."
  },
  {
    "objectID": "Week_7_0/Week7_Solutions.html#titanic",
    "href": "Week_7_0/Week7_Solutions.html#titanic",
    "title": "Week 7 Tasks Solutions",
    "section": "",
    "text": "On 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\nSolution\n\ntitanic &lt;- read_csv(\"titanic.csv\")\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class)\ntitanic$survived &lt;- as.factor(titanic$survived)\nlevels(titanic$survived) &lt;- c(\"Died\", \"Survived\")\ntitanic$gender &lt;- as.factor(titanic$gender)\ntitanic$passenger.class &lt;- as.factor(titanic$passenger.class)\n\n\nggplot(data = titanic, aes(x = survived, y = age, fill = survived)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nTitanic passenger age by survival.\n\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class        Died    Survived\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\n\nBarplot of passenger survival by gender.\n\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\n\nmod.titanic &lt;- logistic_reg()\nmod.titanic &lt;- mod.titanic |&gt; fit (survived ~ gender + passenger.class + age, data = titanic) |&gt;\n  extract_fit_engine()\nmod.titanic %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = survived ~ gender + passenger.class + age, \n    family = stats::binomial, data = data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.54474    0.36537   9.702  &lt; 2e-16 ***\ngendermale       -2.61131    0.18671 -13.986  &lt; 2e-16 ***\npassenger.class2 -1.12216    0.25773  -4.354 1.34e-05 ***\npassenger.class3 -2.32917    0.24089  -9.669  &lt; 2e-16 ***\nage              -0.03330    0.00737  -4.519 6.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  805.29  on 886  degrees of freedom\nAIC: 815.29\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic, show.values = TRUE,\n           title = \"\", show.p = FALSE, value.offset = 0.25)\n\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.07 times those of women, third class passengers’ odds of survival were 0.10 times those of first class passengers, and second class passengers’ odds of survival were 0.33 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.97)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Week 7 Tasks",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\n\n\n\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?"
  },
  {
    "objectID": "about.html#yanny-or-laurel",
    "href": "about.html#yanny-or-laurel",
    "title": "Week 7 Tasks",
    "section": "",
    "text": "This auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the data (yanny.csv) from Moodle and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?"
  },
  {
    "objectID": "about.html#titanic",
    "href": "about.html#titanic",
    "title": "Week 7 Tasks",
    "section": "",
    "text": "On 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) from Moodle for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?"
  }
]