---
title: "Week 7: Generalised Linear Models"
format:
  pdf:
    latex-auto-install: true
  html:    
    code-link: true
    code-fold: true
    code-tools:
      source: false
      toggle: true
    toc: true
    toc-location: left
    toc-title: Contents
    number-sections: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(moderndive)
library(ISLR)
library(skimr)
library(plotly)
library(tidyr)
library(datasets)
library(knitr)
library(janitor)
library(infer)
library(sjPlot)
library(stats)
library(readr)
library(janitor)

knitr::opts_chunk$set(echo = TRUE, comment = NA, out.width = '65%', fig.align = "center", fig.pos = 'H',
                      warning = FALSE, message = FALSE)
```

```{=html}
<style>
p.caption {
  font-size: 1.4em;
}
</style>
```
# Introduction

In Weeks 3 and 4 we looked at modelling data using linear regression models were we had:

-   a **continuous response variable** $y$ and
-   one or more **explanatory variables** $x_1, x_2,\ldots, x_p$, which were **numerical**/**categorical** variables.

Recall that for data $(y_i, x_i), ~ i = 1,\ldots, n$, where $y$ is a continuous response variable, we can write a simple linear regression model as follows:

$$y_i = \alpha + \beta x_i + \epsilon_i, ~~~~ \epsilon_i \sim N(0, \sigma^2),$$ where

-   $y_i$ is the $i^{th}$ observation of the continuous response variable;
-   $\alpha$ is the **intercept** of the regression line;
-   $\beta$ is the **slope** of the regression line;
-   $x_i$ is the $i^{th}$ observation of the explanatory variable; and
-   $\epsilon_i$ is the $i^{th}$ random component.

Thus, the full probability model for $y_i$ given $x_i$ ($y_i | x_i$) can be written as

$$y_i | x_i \sim N(\alpha + \beta x_i, \sigma^2),$$

where the mean $\alpha + \beta x_i$ is given by the deterministic part of the model and the variance $\sigma^2$ by the random part. Hence we make the assumption that the outcomes $y_i$ are normally distributed with mean $\alpha + \beta x_i$ and variance $\sigma^2$. However, what if our response variable $y$ is not a continuous random variable?

## Generalised linear models

The main objective this week is to introduce **Generalised Linear Models (GLMs)**, which extend the linear model framework to response variables that don't follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. Here we shall focus on binary/categorical response variables. The generalised linear model can be written as:

\vspace{-0.5cm}

```{=tex}
\begin{align}
y_i &\sim f(g(\boldsymbol{\mu}_i)) \nonumber \\
\boldsymbol{\mu}_i &= \mathbf{x}_i^\top \boldsymbol{\beta}, \nonumber
\end{align}
```
where the response $y_i$ is predicted through the linear combination $\boldsymbol{\mu}_i$ of explanatory variables by the link function $g(\cdot)$, assuming some distribution $f(\cdot)$ for $y_i$, and $\mathbf{x}_i^\top$ is the $i^{th}$ row of the design matrix $\boldsymbol{X}$. For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as $f(\cdot)$, with corresponding link function equal to the Identity function, that is, $g(\boldsymbol{\mu}_i) = \boldsymbol{\mu}_i$.

What if our response variable $y$ is binary (e.g. yes/no, success/failure, alive/dead)? That is, the independent responses $y_i$ can either be:

-   **binary**, taking the value 1 (say success, with probability $p_i$) or 0 (failure, with probability $1-p_i$) or

-   **binomial**, where $y_i$ is the number of successes in a given number of trials $n_i$, with the probability of success being $p_i$ and the probability of failure being $1-p_i$.

In both cases the distribution of $y_i$ is assumed to be binomial, but in the first case it is Bin$(1,p_i)$ and in the second case it is Bin$(n_i,p_i)$. Hence, a binary response variable $y_i$ has a binomial distribution with corresponding link function $g(\cdot)$ equal to the **logit link** function, that is

$$g(p_i) = \log \left(\frac{p_i}{1 - p_i} \right),$$ which is also referred to as the **log-odds** (since $p_i ~ / ~ 1-p_i$ is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success $p_i$, and as we know probabilities must be between 0 and 1 $\left(p_i \in [0, 1]\right)$. So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that

$$p_i = \mathbf{x}_i^\top \boldsymbol{\beta},$$ we would need to ensure that in some way $\mathbf{x}_i^\top \boldsymbol{\beta} \in [0, 1]$, that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that

$$\log \left(\frac{p_i}{1 - p_i} \right) = \mathbf{x}_i^\top \boldsymbol{\beta},$$

no restrictions need to be in place on our estimates of the parameter vector $\boldsymbol{\beta}$, since the inverse of the logit link function will always gives us valid probabilities since

$$p_i = \frac{\exp\left(\mathbf{x}_i^\top \boldsymbol{\beta}\right)}{1 + \exp\left(\mathbf{x}_i^\top \boldsymbol{\beta}\right)} ~~~ \in [0, 1].$$ This linear regression model with a binary response variable is referred to as **logistic regression**. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.

| **Model** |                   **Random component**                    |                               **Systematic component**                                |                 **Link function**                  |
|:---------:|:---------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:--------------------------------------------------:|
|  Normal   | $y_i\overset{\text{indep}}\sim \mbox{N}(\mu_i,\sigma^2),$ | $\boldsymbol{x}_i^\top\boldsymbol{\beta} =\beta_0 + \beta_1x_i + \beta_2x_i + \ldots$ |                  $g(\mu_i)=\mu_i$                  |
| Logistic  |    $y_i\overset{\text{indep}}\sim \mbox{Bin}(1,p_i),$     | $\boldsymbol{x}_i^\top\boldsymbol{\beta} =\beta_0+ \beta_1x_i + \beta_2x_i + \ldots$  | $g(\mu_i) = \log \left( \frac{p_i}{1-p_i} \right)$ |

## Required R packages {.unnumbered}

Before we proceed, load all the packages needed for this week:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(moderndive)
library(gapminder)
library(sjPlot)
library(stats)
library(readr)
library(janitor)
library(tidymodels)
```

# Logistic regression with one numerical explanatory variable

Here we shall begin by fitting a logistic regression model with one numerical explanatory variable. Let's return to the `evals` data from the `moderndive` package that we examined in Week 3.

## Teaching evaluation scores

Student feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see [Economics of Education Review](https://www.journals.elsevier.com/economics-of-education-review/) for details. Here, we shall look at a study from student evaluations of $n=463$ professors from The University of Texas at Austin.

Previously, we looked at **teaching score** as our continuous response variable and **beauty score** as our explanatory variable. Now we shall consider **gender** as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in **gender** by **age** of the teaching instructors within the `evals` data set.

First, let's start by selecting the variables of interest from the `evals` data set:

```{r evals1, echo = c(1), eval = TRUE}
evals.gender <- evals |>
                  select(gender, age)
head(evals.gender)
```

Now, let's look at a boxplot of `age` by `gender` to get an initial impression of the data:

```{r, fig.cap = "Teaching instructor age by gender.", fig.align = "center", out.width = "50%"}
ggplot(data = evals.gender, aes(x = gender, y = age, fill = gender)) +
  geom_boxplot() +
  labs(x = "Gender", y = "Age") +
  theme(legend.position = "none")
```

Here we can see that the age of male teaching instructors tends to be higher than that of their female counterparts. Now, let's fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female.

## Log-odds

To fit a logistic regression model we will use the generalised linear model function `glm`, which acts in a very similar manner to the `lm` function we have used previously. The logistic regression model with **gender** as the response and **age** as the explanatory variable is given by:

```{r model1, echo = TRUE, eval = TRUE}
model <- logistic_reg() |> 
  set_engine("glm")
model <- model |> 
  fit(gender ~ age, data = evals.gender) |>
  extract_fit_engine()
```

This model uses the **logit link** function. Now, let's take a look at the summary produced from our logistic regression model:

```{r mod1sum, echo = TRUE, eval = TRUE}
model |>
  summary()
```

```{r mod1coefs, echo = FALSE, eval = TRUE}
mod1coefs <- round(coef(model), 2)
```

Firstly, the baseline category for our binary response is `female`. This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the `levels` function:

```{r levels, echo = TRUE, eval = TRUE}
levels(evals.gender$gender)
```

This means that estimates from the logistic regression model are for a change on the **log-odds** scale for `males` in comparison to the response baseline `females`. That is

```{=tex}
\begin{align}
\ln\left(\frac{p}{1-p}\right) &= \alpha + \beta \cdot \textrm{age} = `r mod1coefs[1]` + `r mod1coefs[2]` \cdot \textrm{age}, \nonumber
\end{align}
```
where $p = \textrm{Prob}\left(\textrm{Male}\right)$ and $1 - p = \textrm{Prob}\left(\textrm{Female}\right)$. Hence, the **log-odds** of the instructor being male increase by `r mod1coefs[2]` for every one unit increase in `age`. This provides us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:

```{r, echo = c(1, 2, 3, 5, 7), eval = TRUE}
mod.coef.logodds <- model |>
                      summary() |>
                      coef()
```

```{r, echo = c(1, 3), eval = TRUE}
age.logodds.lower <- (mod.coef.logodds["age", "Estimate"] 
                        - 1.96 * mod.coef.logodds["age", "Std. Error"])
age.logodds.lower
age.logodds.upper <- (mod.coef.logodds["age", "Estimate"] 
                        + 1.96 * mod.coef.logodds["age", "Std. Error"])
age.logodds.upper
```

Hence the point estimate for the log-odds is `r mod1coefs[2]`, which has a corresponding 95% confidence interval of (`r round(age.logodds.lower, 2)`, `r round(age.logodds.upper, 2)`). This can be displayed graphically using the `plot_model` function from the `sjPlot` package by simply passing our `model` as an argument:

```{r, fig.cap = "The log-odds of age for male instructors.", fig.align="center", out.width = "50%"}
plot_model(model, show.values = TRUE, transform = NULL,
           title = "Log-Odds (Male instructor)", show.p = FALSE)
```

Some of the interesting arguments that can be passed to the `plot_model` function are:

-   `show.values = TRUE/FALSE`: Whether the log-odds/odds values should be displayed;
-   `show.p = TRUE/FALSE`: Adds asterisks that indicate the significance level of estimates to the value labels;
-   `transform`: A character vector naming the function that will be applied to the estimates. The default transformation uses `exp` to display the odds ratios, while `transform = NULL` displays the log-odds; and
-   `vline.color`: colour of the vertical "zero effect" line.

Further details on using `plot_model` can be found [here](https://strengejacke.wordpress.com/2017/10/23/one-function-to-rule-them-all-visualization-of-regression-models-in-rstats-w-sjplot/). Now, let's add the estimates of the log-odds to our data set:

```{r, echo = c(1)}
evals.gender <- evals.gender |>
                  mutate(logodds.male = predict(model))
head(evals.gender)
```

## Odds

Typically we would like to work on the **odds** scale as it is easier to interpret an odds-ratio as opposed to the log-odds-ratio. To obtain the odds we simply exponentiate the log-odds, that is

```{=tex}
\begin{align}
\frac{p}{1-p} &= \exp\left(\alpha + \beta \cdot \textrm{age} \right), \nonumber
\end{align}
```
```{r, echo = TRUE, eval = TRUE}
model |>
 coef() |>
  exp()
```

```{r, echo = FALSE, eval = TRUE}
mod1.odds <- model |>
              coef() |>
              exp()
```

On the odds scale, the value of the intercept (`r round(mod1.odds["(Intercept)"], 2)`) gives the odds of a teaching instructor being male given their `age = 0`, which is obviously not a viable age for a teaching instructor, and hence why this value is very close to zero. For `age` we have an odds of `r round(mod1.odds[("age")], 2)`, which indicates that for every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of `r round(mod1.odds[("age")], 2)`. So how is this calculated? Let's look at the odds-ratio obtained from instructors aged 51 and 52 years old, that is, a one unit difference:

```{=tex}
\begin{align}
\frac{\mbox{Odds}_{\mbox{age=52}}}{\mbox{Odds}_{\mbox{age=51}}} = \left(\frac{\frac{p_{\mbox{age=52}}}{1 - p_{\mbox{age=52}}}}{\frac{p_{\mbox{age=51}}}{1 - p_{\mbox{age=51}}}}\right) = \frac{\exp\left(\alpha + \beta \cdot 52\right)}{\exp\left(\alpha + \beta \cdot 51\right)} = \exp\left(\beta \cdot (52 - 51)\right) = \exp\left(`r mod1coefs[2]`\right) = `r round(exp(mod1coefs[2]), 2)`. \nonumber
\end{align}
```
For example, the odds of a teaching instructor who is 45 years old being male is given by

```{=tex}
\begin{align}
\frac{p}{1-p} &= \exp\left(\alpha + \beta \cdot \textrm{age}\right) = \exp\left(`r mod1coefs[1]` + `r mod1coefs[2]` \cdot 45\right) = `r round(exp(mod.coef.logodds["(Intercept)", "Estimate"] + mod.coef.logodds["age", "Estimate"] * 45), 2)`. \nonumber
\end{align}
```
This can be interpreted as the chances of an instructor who is 45 being male are 15% greater than them being female. We can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of our log-odds interval:

```{r, echo = c(1, 3), eval = TRUE}
age.odds.lower <- exp(age.logodds.lower)
age.odds.lower
age.odds.upper <- exp(age.logodds.upper)
age.odds.upper
```

Hence the point estimate for the odds is `r round(mod1.odds["age"], 2)`, which has a corresponding 95% confidence interval of (`r round(age.odds.lower, 2)`, `r round(age.odds.upper, 2)`). This can be displayed graphically using the `plot_model` function from the `sjPlot` package by simply passing our `model` as an argument as well as removing `transform = NULL` (the default transformation is exponential):

```{r, fig.cap = "The odds of age for male instructors.", fig.align = "center", out.width = "50%"}
plot_model(model, show.values = TRUE,
           title = "Odds (Male instructor)", show.p = FALSE, axis.lim = c(1, 1.5))
```

**Note**: `axis.lim` is used to zoom in on the 95% confidence interval. The `confint()` function can also be used to compute confidence intervals (`confint(model)` for example).

Now, let's add the estimates of the odds to our data set:

```{r, echo = c(1)}
evals.gender <- evals.gender |>
                  mutate(odds.male = exp(logodds.male))
head(evals.gender)
```

## Probabilities

We can obtain the probability $p = \textrm{Prob}(\textrm{Male})$ using the following transformation:

```{=tex}
\begin{align}
p &= \frac{\exp\left(\alpha + \beta \cdot \textrm{age} \right)}{1 + \exp\left(\alpha + \beta \cdot \textrm{age} \right)}. \nonumber
\end{align}
```
For example, the probability of a teaching instructor who is 52 years old being male is

```{=tex}
\begin{align}
p &= \frac{\exp\left(\alpha + \beta \cdot \textrm{age} \right)}{1 + \exp\left(\alpha + \beta \cdot \textrm{age} \right)}
=\frac{\exp\left(`r mod.coef.logodds["(Intercept)", "Estimate"]` + `r mod.coef.logodds["age", "Estimate"]`\cdot 52 \right)}{1 + \exp\left(`r mod.coef.logodds["(Intercept)", "Estimate"]` + `r mod.coef.logodds["age", "Estimate"]`\cdot 52 \right)} 
= 0.64, \nonumber
\end{align}
```
which can be computed in R as follows:

```{r}
p.num <- (exp(mod.coef.logodds["(Intercept)", "Estimate"] 
              + mod.coef.logodds["age", "Estimate"] * 52))
p.denom <- 1 + p.num
p.num / p.denom
```

The `plogis()` function from the `stats` library can also be used to obtain probabilities from the log-odds:

```{r}
plogis(mod.coef.logodds["(Intercept)", "Estimate"] 
        + mod.coef.logodds["age", "Estimate"] * 52)
```

Let's add the probabilities to our data, which is done using the `fitted()` function:

```{r, echo = c(1)}
evals.gender <- evals.gender |>
                  mutate(probs.male = fitted(model))
```

**Note**: `predict(model, type = "response")` will also provide the estimated probabilities.

Finally, we can plot the probability of being male using the `geom_smooth()` function by giving `method = "glm"` and `methods.args = list(family = "binomial")` as follows:

<!-- ```{r, echo = TRUE, eval = TRUE, fig.cap = "Probability of teaching instructor being male by age.", fig.align = "center", out.width = "60%"} -->

<!-- ggplot(data = evals.gender, aes(x = age, y = probs.male)) + -->

<!--   geom_dotplot(dotsize = 0.6, alpha = 0.2, aes(fill = gender), -->

<!--                 binwidth = 1, stackgroups = TRUE) + -->

<!--   geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) + -->

<!--   labs(x = "Age", y = "Probability of instructor being male") -->

<!-- ``` -->

<!--  -->

```{r, echo = TRUE, eval = TRUE, fig.cap = "Probability of teaching instructor being male by age.", fig.align = "center", out.width = "60%"}
ggplot(data = evals.gender, aes(x = age, y = probs.male)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(x = "Age", y = "Probability of instructor being male")
```

<!-- **Note**: the ages of all teaching instructors have been superimposed as a dotplot using `geom_dotplot()`. -->

\newpage

The `plot_model()` function from the `sjPlot` package can also produce the estimated probabilities by `age` as follows:

```{r, echo = TRUE, eval = TRUE, fig.cap = "Probability of teaching instructor being male by age.", fig.align = "center", results = FALSE, out.width = "50%"}
plot_model(model, type = "pred", title = "", terms="age [all]", axis.title = c("Age", "Prob. of instructor being male"))
```

# Logistic regression with one categorical explanatory variable

Instead of having a numerical explanatory variable such as `age`, let's now use the binary categorical variable `ethnicity` as our explanatory variable.

```{r evals2, echo = c(1), eval = TRUE}
evals.ethnic <- evals |>
                  select(gender, ethnicity)
head(evals.ethnic)
```

Now, let's look at a barplot of the proportion of males and females by `ethnicity` to get an initial impression of the data.

```{r}
evals.ethnic |>
  tabyl(ethnicity, gender) |>
  adorn_percentages() |>
  adorn_pct_formatting() |>
  adorn_ns() # To show original counts
```

```{r, fig.cap = "Barplot of teaching instructors' gender by ethnicity.", fig.align = "center", out.width = "48%"}
ggplot(evals.ethnic, aes(x = gender, group = ethnicity)) +
    geom_bar(aes(y = after_stat(prop), fill = ethnicity), stat = "count", position = "dodge") +
    labs(y = "Proportion", fill = "Ethnicity")
```

We can see that a larger proportion of instructors in the `minority` ethnic group are female (56.3% vs 43.8%), while the `not minority` ethnic group is comprised of more male instructors (60.2% vs 39.8%). Now we shall fit a logistic regression model to determine whether the gender of a teaching instructor can be predicted from their ethnicity.

## Log-odds

The logistic regression model is given by:

```{r model2, echo = TRUE, eval = TRUE}
model.ethnic <- glm(gender ~ ethnicity, data = evals.ethnic,
                      family = binomial(link = "logit"))
```

```{r mod2sum, echo = TRUE, eval = TRUE}
model.ethnic |>
  summary()
```

```{r mod2coefs, echo = FALSE, eval = TRUE}
mod2coefs <- round(coef(model.ethnic), 2)
```

Again, the baseline category for our binary response is `female`. Also, the baseline category for our explanatory variable is `minority`, which, like `gender`, is done alphabetically by default by R:

```{r}
levels(evals.ethnic$ethnicity)
```

This means that estimates from the logistic regression model are for a change on the **log-odds** scale for `males` ($p = \textrm{Prob}(\textrm{Males})$) in comparison to the response baseline `females`. That is

```{=tex}
\begin{align}
\ln\left(\frac{p}{1-p}\right) &= \alpha + \beta \cdot \textrm{ethnicity} = `r mod2coefs[1]` + `r mod2coefs[2]` \cdot \mathbb{I}_{\mbox{ethnicity}}(\mbox{not minority}), \nonumber
\end{align}
```
where $\mathbb{I}_{\mbox{ethnicity}}(\mbox{not minority})$ is an indicator function. Hence, the **log-odds** of an instructor being male increase by `r mod2coefs[2]` if they are in the ethnicity group `not minority`. This provides us with a point estimate of how the log-odds changes with ethnicity, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done as follows:

```{r, echo = c(1, 2, 3, 5, 7), eval = TRUE}
mod.ethnic.coef.logodds <- model.ethnic |>
                            summary() |>
                            coef()
```

```{r, echo = c(1, 3), eval = TRUE}
ethnic.logodds.lower <- (mod.ethnic.coef.logodds["ethnicitynot minority", "Estimate"] 
                          - 1.96 *
                           mod.ethnic.coef.logodds["ethnicitynot minority", "Std. Error"])
ethnic.logodds.lower
ethnic.logodds.upper <- (mod.ethnic.coef.logodds["ethnicitynot minority", "Estimate"] 
                          + 1.96 *
                           mod.ethnic.coef.logodds["ethnicitynot minority", "Std. Error"])
ethnic.logodds.upper
```

Hence the point estimate for the log-odds is `r mod2coefs[2]`, which has a corresponding 95% confidence interval of (`r round(ethnic.logodds.lower, 2)`, `r round(ethnic.logodds.upper, 2)`). This can be displayed graphically using the `plot_model` function from the `sjPlot` package by simply passing our `model` as an argument:

```{r, fig.cap = "The log-odds for male instructors by ethnicity (not a minority).", fig.align="center", out.width = "50%"}
plot_model(model.ethnic, show.values = TRUE, transform = NULL, 
           title = "Log-Odds (Male instructor)", show.p = FALSE)
```

Now, let's add the estimates of the log-odds to our data set:

```{r, echo = c(1)}
evals.ethnic <- evals.ethnic |>
                  mutate(logodds.male = predict(model.ethnic))
head(evals.ethnic)
```

## Odds

On the **odds** scale the regression coefficients are given by

```{r, echo = TRUE, eval = TRUE}
model.ethnic |>
 coef() |>
  exp()
```

```{r, echo = FALSE, eval = TRUE}
mod.ethnic.odds <- model.ethnic |>
                    coef() |>
                    exp()
```

The `(Intercept)` gives us the odds of the instructor being male given that they are in the `minority` ethnic group, that is, `r round(mod.ethnic.odds["(Intercept)"], 2)` (the indicator function is zero in that case). The odds of the instructor being male given they are in the `not minority` ethnic group are `r round(mod.ethnic.odds["ethnicitynot minority"], 2)` times greater than the odds if they were in the `minority` ethnic group.

Before moving on, let's take a look at how these values are computed. First, the odds of the instructor being male given that they are in the `minority` ethnic group can be obtained as follows:

\vspace{-0.6cm}

```{=tex}
\begin{align}
\frac{p_{\mbox{minority}}}{1 - p_{\mbox{minority}}} = \exp\left(\alpha\right) = \exp\left(`r round(mod2coefs[1], 2)`\right) = `r round(exp(round(mod2coefs[1], 2)), 2)`. \nonumber
\end{align}
```
```{r, echo = c(1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13), eval = TRUE}
# the number of instructors in the minority
pmin <- evals.ethnic |>
              filter(ethnicity == "minority") |>
              summarize(n()) |>
              pull()

# the number of male instructors in the minority
pmin.male <- evals.ethnic |>
              filter(ethnicity == "minority", gender == "male") |>
              summarize(n()) |>
              pull()

# the proportion/probability of males in the minority
prob.min.male <- pmin.male / pmin
# the odds of an instructor being male given they are in the minority
odds.min.male <- prob.min.male / (1 - prob.min.male)
odds.min.male
```

Now, the odds-ratio of an instructor being male in the `not minority` compared to the `minority` ethnic group is found as follows:

\vspace{-0.6cm}

```{=tex}
\begin{align}
\frac{\mbox{Odds}_{\mbox{not minority}}}{\mbox{Odds}_{\mbox{minority}}} = \frac{\frac{p_{\mbox{not minority}}}{1 - p_{\mbox{not minority}}}}{\frac{p_{\mbox{minority}}}{1 - p_{\mbox{minority}}}} &= \frac{\exp\left(\alpha + \beta\right)}{\exp\left(\alpha\right)} = \exp\left(\alpha + \beta - \alpha\right) = \exp\left(\beta\right) = \exp\left(`r round(mod2coefs[2], 2)` \right) = `r round(exp(round(mod2coefs[2], 2)), 2)`. \nonumber 
\end{align}
```
```{r, echo = c(1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13), eval = TRUE}
# the number of instructors not in the minority
pnotmin <- evals.ethnic |>
              filter(ethnicity == "not minority") |>
              summarize(n()) |>
              pull()

# the number of male instructors not in the minority
pnotmin.male <- evals.ethnic |>
              filter(ethnicity == "not minority", gender == "male") |>
              summarize(n()) |>
              pull()

# the proportion/probability of males not in the minority
prob.notmin.male <- pnotmin.male / pnotmin
# the odds of an instructor being male given they are not in the minority
odds.notmin.male <- prob.notmin.male / (1 - prob.notmin.male)
odds.ratio.notmin <- odds.notmin.male / odds.min.male
odds.ratio.notmin
```

We can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of the log-odds interval:

```{r, echo = c(1, 3), eval = TRUE}
ethnic.odds.lower <- exp(ethnic.logodds.lower)
ethnic.odds.lower
ethnic.odds.upper <- exp(ethnic.logodds.upper)
ethnic.odds.upper
```

Hence the point estimate for the odds-ratio is `r round(mod.ethnic.odds["ethnicitynot minority"], 2)`, which has a corresponding 95% confidence interval of (`r round(ethnic.odds.lower, 2)`, `r round(ethnic.odds.upper, 2)`). Again, we can display this graphically using the `plot_model` function from the `sjPlot` package:

```{r, fig.cap = "The odds-ratio of a male instructor given they are in the `not minority` group.", fig.align="center", out.width = "50%"}
plot_model(model.ethnic, show.values = TRUE,
           title = "Odds (Male instructor)", show.p = FALSE)
```

Now, let's add the estimates of the odds to our data set:

```{r, echo = c(1)}
evals.ethnic <- evals.ethnic |>
                  mutate(odds.male = exp(logodds.male))
head(evals.ethnic)
```

## Probabilities

The probabilities of an instructor being male given they are in the `minority` and `not minority` groups are

```{r}
plogis(mod.ethnic.coef.logodds["(Intercept)", "Estimate"])
plogis(mod.ethnic.coef.logodds["(Intercept)", "Estimate"] +
         mod.ethnic.coef.logodds["ethnicitynot minority", "Estimate"])
```

```{r, echo = FALSE, eval = TRUE}
ps <- c(plogis(mod.ethnic.coef.logodds["(Intercept)", "Estimate"]),
          plogis(mod.ethnic.coef.logodds["(Intercept)", "Estimate"] +
                   mod.ethnic.coef.logodds["ethnicitynot minority", "Estimate"]))
```

Hence, the probabilities of an instructor being male given they are in the `minority` and `not minority` ethnic groups are `r round(ps[1], 3)` and `r round(ps[2], 3)`, respectively.

Let's add the probabilities to our data:

```{r, echo = c(1)}
evals.ethnic <- evals.ethnic |>
                  mutate(probs.male = fitted(model.ethnic))
```

Finally, we can use the `plot_model()` function from the `sjPlot` package to produce the estimated probabilities by `ethnicity` as follows:

```{r, echo = TRUE, eval = TRUE, fig.cap = "Probability of teaching instructor being male by ethnicity.", fig.align = "center", results = FALSE, out.width = "50%"}
plot_model(model.ethnic, type = "pred", terms = "ethnicity", axis.title = c("Ethnicity", "Prob. of instructor being male"), title = " ")
```
